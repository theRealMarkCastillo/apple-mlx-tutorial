# Production-Ready NLP with Real Datasets

This directory contains production-ready examples using real-world datasets.

## üöÄ Quick Start

### 1. Install Additional Dependencies

```bash
# Make sure you're in the virtual environment
source .venv/bin/activate

# Install datasets library
pip install datasets
```

### 2. Run Production Example

```bash
# Start Jupyter notebooks
cd notebooks
jupyter notebook

# Open 04_Complete_Pipeline.ipynb
```

This notebook demonstrates the full production pipeline:
- Download 5,000 IMDB movie reviews
- Clean and preprocess the data
- Build vocabulary
- Train LSTM sentiment classifier
- Evaluate with comprehensive metrics
- Save versioned model
- Show demo predictions

**Expected runtime**: ~10-15 minutes on M1/M2/M3 Mac

## üìä What Datasets Are Available?

### Sentiment Analysis
- **IMDB Reviews**: 50K movie reviews (positive/negative)
- **Amazon Reviews**: Millions of product reviews (1-5 stars)
- **Yelp Reviews**: 560K business reviews (1-5 stars)
- **Twitter Sentiment140**: 1.6M tweets

### Intent Classification
- **ATIS**: 5,871 flight booking queries (26 intents)
- **SNIPS**: 16K+ queries (7 intents: weather, music, etc.)
- **Banking77**: 13K banking queries (77 fine-grained intents)

### Text Generation
- **WikiText**: 100M+ tokens from Wikipedia
- **OpenWebText**: 38GB web text
- **DailyDialog**: 13K conversations
- **PersonaChat**: Conversations with personalities

## üìö Documentation

See the comprehensive guides in `docs/`:

1. **[DATASETS_AND_PREPROCESSING.md](docs/DATASETS_AND_PREPROCESSING.md)**
   - How to download and load datasets
   - Data cleaning pipelines
   - Tokenization and formatting
   - Quality filtering
   - Data augmentation

2. **[PRODUCTION_BEST_PRACTICES.md](docs/PRODUCTION_BEST_PRACTICES.md)**
   - Model versioning
   - Experiment tracking
   - Performance optimization
   - REST API deployment
   - Monitoring and logging
   - Security best practices

## üîß Customizing the Production Example

### Change Dataset Size

```python
# In the notebook data loading cell:
train_texts, train_labels, test_texts, test_labels = load_and_clean_imdb(
    max_samples=25000  # Change this number
)
```

### Change Model Architecture

```python
# Modify config dictionary in the notebook:
config = {
    'vocab_size': 10000,      # Larger vocabulary
    'embedding_dim': 256,     # Bigger embeddings
    'hidden_dim': 512,        # More LSTM capacity
    'dropout': 0.5,           # Higher dropout
    'epochs': 10,             # More training
}
```

### Use Different Dataset

```python
# Replace load_and_clean_imdb() in the notebook with:
from datasets import load_dataset

# For Amazon reviews:
dataset = load_dataset("amazon_us_reviews", "All_Beauty")

# For Yelp reviews:
dataset = load_dataset("yelp_review_full")

# For Twitter:
dataset = load_dataset("sentiment140")
```

## üìà Expected Results

With the default configuration (5K IMDB samples):

```
Training Progress:
Epoch    Train Loss   Val Loss     Val Acc
------------------------------------------------
1        0.5123       0.4567       0.78
2        0.3456       0.4012       0.82
3        0.2345       0.3890       0.84
4        0.1678       0.4001       0.85
5        0.1234       0.4123       0.85

Test Accuracy: ~85%
```

With full IMDB dataset (25K samples, 10 epochs):
- **Test Accuracy**: ~88-92%
- **Training time**: ~1-2 hours on M1 Mac

## üè≠ Production Deployment

### 1. Train Full Model

Run the `04_Complete_Pipeline.ipynb` notebook with `max_samples=25000` to train on the full dataset. The notebook will save the model to the `production_models/` directory.

### 2. Load and Serve Model

You can use the `mlx_nlp_utils.py` module to load and serve the model:

```python
from notebooks.mlx_nlp_utils import SentimentPredictor, SentimentClassifier, Tokenizer
import mlx.core as mx

# Load model
model = SentimentClassifier(vocab_size=5000, embedding_dim=128, hidden_dim=256)
model.load_weights('production_models/v_YYYYMMDD_HHMMSS/model.npz')

# Load tokenizer
tokenizer = Tokenizer()
tokenizer.load('production_models/v_YYYYMMDD_HHMMSS/vocab.json')

# Create predictor
predictor = SentimentPredictor(model, tokenizer)

# Predict
result = predictor.predict("This movie is amazing!")
print(result)
# {'sentiment': 'Positive', 'confidence': 0.95, 'probabilities': {...}}
```

### 3. Deploy as REST API

See `docs/PRODUCTION_BEST_PRACTICES.md` for complete REST API server code.

```python
# Example using FastAPI
from fastapi import FastAPI
from notebooks.mlx_nlp_utils import SentimentPredictor

app = FastAPI()
# ... setup predictor ...

@app.post("/predict")
async def predict(text: str):
    return predictor.predict(text)
```

### 4. Docker Deployment

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY production_models/ ./production_models/
COPY notebooks/mlx_nlp_utils.py .
COPY app.py .
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

## üß™ Running Experiments

### Hyperparameter Search

You can use Python loops in the notebook to run grid searches:

```python
# In a notebook cell:
learning_rates = [0.0001, 0.001, 0.01]
results = []

for lr in learning_rates:
    print(f"\n=== Training with LR={lr} ===")
    config['learning_rate'] = lr
    # ... run training function ...
    # ... append metrics to results ...
```

### Compare Model Versions

The `mlx_nlp_utils.py` module includes versioning tools:

```python
from notebooks.mlx_nlp_utils import ModelVersioning

versioning = ModelVersioning()

# List all versions
versions = versioning.list_versions()
for v in versions:
    print(f"{v['version_id']}: Accuracy {v['metrics']['accuracy']:.2%}")

# Find best
best = max(versions, key=lambda v: v['metrics']['accuracy'])
print(f"\nBest model: {best['version_id']}")
print(f"Accuracy: {best['metrics']['accuracy']:.2%}")
```

## üìä Performance Benchmarks

### IMDB Sentiment Analysis

| Samples | Epochs | Batch Size | Accuracy | Training Time (M1) |
|---------|--------|------------|----------|-------------------|
| 1,000   | 5      | 32         | ~78%     | ~2 min            |
| 5,000   | 5      | 32         | ~85%     | ~10 min           |
| 10,000  | 10     | 64         | ~88%     | ~30 min           |
| 25,000  | 10     | 64         | ~90%     | ~90 min           |

### Inference Speed (M1 Mac)

| Batch Size | Latency (ms) | Throughput (samples/sec) |
|------------|--------------|-------------------------|
| 1          | ~5 ms        | 200                     |
| 32         | ~50 ms       | 640                     |
| 64         | ~90 ms       | 711                     |

## üéØ Next Steps

1. **Experiment with datasets**: Try different domains (products, tweets, news)
2. **Tune hyperparameters**: Learning rate, model size, dropout
3. **Data augmentation**: Use techniques from the preprocessing guide
4. **Deploy**: Set up REST API with monitoring
5. **Scale**: Use larger datasets and longer training

## üìñ Learn More

- [Datasets & Preprocessing Guide](docs/DATASETS_AND_PREPROCESSING.md) - Complete data pipeline
- [Production Best Practices](docs/PRODUCTION_BEST_PRACTICES.md) - Deployment & monitoring
- [Sentiment Analysis Guide](docs/SENTIMENT_ANALYSIS_GUIDE.md) - Deep dive into sentiment
- [MLX Framework Guide](docs/MLX_FRAMEWORK_GUIDE.md) - Optimize for Apple Silicon

## üÜò Troubleshooting

**Out of memory during training?**
- Reduce batch_size in notebook config: `config['batch_size'] = 16`
- Reduce model size: `config['hidden_dim'] = 128`

**Training too slow?**
- Increase batch_size: `config['batch_size'] = 64`
- Use fewer samples for initial experiments

**Poor accuracy?**
- Train longer: `config['epochs'] = 10`
- Use more training data: `max_samples=25000`
- Increase model capacity: `config['hidden_dim'] = 512`

**Dataset download failing?**
- Check internet connection
- Try: `export HF_DATASETS_OFFLINE=0`
- Clear cache: `rm -rf ~/.cache/huggingface/datasets`

---

**Ready to build production NLP systems!** üöÄ
