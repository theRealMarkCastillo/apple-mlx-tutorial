{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4b7baf",
   "metadata": {},
   "source": [
    "# ðŸš€ Building NanoGPT: A Complete Transformer from Scratch\n",
    "\n",
    "## What You'll Learn\n",
    "- **Build a real GPT**: From the \"Attention is All You Need\" paper\n",
    "- **Multi-Head Attention**: Why 8 heads are better than 1\n",
    "- **Residual Connections**: The secret to training deep networks\n",
    "- **Train on Shakespeare**: Generate text in Elizabethan English\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input Text\n",
    "    â†“\n",
    "Token + Position Embeddings\n",
    "    â†“\n",
    "[Transformer Block] Ã— N layers\n",
    "â”‚   â”œâ”€ Multi-Head Attention (with causal mask)\n",
    "â”‚   â”œâ”€ Residual Connection + LayerNorm\n",
    "â”‚   â”œâ”€ Feed-Forward Network\n",
    "â”‚   â””â”€ Residual Connection + LayerNorm\n",
    "    â†“\n",
    "Output Head (predict next token)\n",
    "```\n",
    "\n",
    "This is the **Decoder-Only** architecture used by GPT-3, GPT-4, Llama, and Claude.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69dbbe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ–¥ï¸  Hardware Acceleration Check:\n",
      "   Device: Device(gpu, 0)\n",
      "   âœ… Using Apple Silicon GPU (Metal)\n",
      "   â„¹ï¸  MLX automatically optimizes for the GPU's Unified Memory.\n",
      "   â„¹ï¸  Note: While Apple Silicon has an NPU (Neural Engine), MLX primarily\n",
      "       uses the powerful GPU for general-purpose training tasks like LSTMs.\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our utilities\n",
    "from mlx_nlp_utils import print_device_info, load_sample_corpus, create_char_vocab\n",
    "\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06025da0",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention\n",
    "\n",
    "Instead of one attention head, we split the embedding into $h$ heads, compute attention for each, and concatenate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7266e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        \n",
    "        # Key, Query, Value projections combined\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Calculate Q, K, V\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = mx.split(qkv, 3, axis=-1)\n",
    "        \n",
    "        # Reshape for multi-head: (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)\n",
    "        q = q.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        k = k.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        scores = (q @ k.transpose(0, 1, 3, 2)) * scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "            \n",
    "        attn = mx.softmax(scores, axis=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Weighted sum\n",
    "        y = (attn @ v)\n",
    "        \n",
    "        # Reassemble heads: (B, n_head, T, head_dim) -> (B, T, C)\n",
    "        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)\n",
    "        \n",
    "        return self.resid_dropout(self.c_proj(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3306d",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network\n",
    "\n",
    "A simple MLP applied to each position independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e73d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6794e",
   "metadata": {},
   "source": [
    "## 3. The Transformer Block\n",
    "\n",
    "Combining Attention and Feed-Forward with Residuals and Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c67278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_head, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadAttention(n_head, n_embd, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        # Residual connections: x + layer(norm(x))\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e8ad1",
   "metadata": {},
   "source": [
    "## 4. The GPT Model\n",
    "\n",
    "Putting it all together: Embeddings -> Blocks -> Output Head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "995f901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_head, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(n_head, n_embd, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __call__(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, ..., T-1]\n",
    "        pos = mx.arange(0, T)\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Causal Mask (ensure we can't see the future)\n",
    "        mask = nn.MultiHeadAttention.create_additive_causal_mask(T)\n",
    "        mask = mask.astype(x.dtype)\n",
    "        \n",
    "        # Run through blocks\n",
    "        for block in self.blocks.layers:\n",
    "            x = block(x, mask)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c84c4",
   "metadata": {},
   "source": [
    "## 5. Training on Shakespeare\n",
    "\n",
    "Let's train our model on a small corpus to see it learn language patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare a tiny corpus\n",
    "sample_text = \"\"\"To be or not to be that is the question.\n",
    "Whether tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune\n",
    "Or to take arms against a sea of troubles\"\"\"\n",
    "\n",
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(sample_text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Encode the text\n",
    "data = mx.array([char_to_idx[ch] for ch in sample_text])\n",
    "print(f\"\\nEncoded text shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb30f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training batches\n",
    "def get_batch(data, block_size=32, batch_size=4):\n",
    "    \"\"\"Generate a batch of input/target pairs\"\"\"\n",
    "    ix = mx.random.randint(0, len(data) - block_size, (batch_size,))\n",
    "    # Convert to python integers for slicing\n",
    "    indices = ix.tolist()\n",
    "    x = mx.stack([data[i:i+block_size] for i in indices])\n",
    "    y = mx.stack([data[i+1:i+block_size+1] for i in indices])\n",
    "    return x, y\n",
    "\n",
    "# Test batch generation\n",
    "block_size = 32\n",
    "xb, yb = get_batch(data, block_size, batch_size=2)\n",
    "print(f\"Input batch shape: {xb.shape}\")  # (2, 32)\n",
    "print(f\"Target batch shape: {yb.shape}\")  # (2, 32)\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Input:  {''.join([idx_to_char[int(i)] for i in xb[0][:20]])}\")\n",
    "print(f\"Target: {''.join([idx_to_char[int(i)] for i in yb[0][:20]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our NanoGPT\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'n_layer': 2,        # 2 transformer blocks (small for demo)\n",
    "    'n_head': 4,         # 4 attention heads\n",
    "    'n_embd': 64,        # 64-dimensional embeddings\n",
    "    'block_size': block_size,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = GPT(**config)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(x.size for k, x in model.parameters().items())\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = optim.AdamW(learning_rate=3e-4)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape\n",
    "    logits_flat = logits.reshape(B * T, C)\n",
    "    targets_flat = y.reshape(B * T)\n",
    "    return mx.mean(nn.losses.cross_entropy(logits_flat, targets_flat))\n",
    "\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "print(\"Training NanoGPT...\")\n",
    "losses = []\n",
    "\n",
    "for step in range(100):  # Small number for demo\n",
    "    # Get batch\n",
    "    xb, yb = get_batch(data, block_size, batch_size=4)\n",
    "    \n",
    "    # Forward + backward\n",
    "    loss, grads = loss_and_grad_fn(model, xb, yb)\n",
    "    optimizer.update(model, grads)\n",
    "    mx.eval(model.parameters(), optimizer.state)\n",
    "    \n",
    "    losses.append(float(loss))\n",
    "    \n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"Step {step+1:3d} | Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721b5e3",
   "metadata": {},
   "source": [
    "## 6. Text Generation\n",
    "\n",
    "Now let's generate text with our trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_str, max_new_tokens=100, temperature=1.0):\n",
    "    \"\"\"Generate text from the model\"\"\"\n",
    "    # Encode the starting string\n",
    "    context = mx.array([char_to_idx[ch] for ch in start_str]).reshape(1, -1)\n",
    "    \n",
    "    generated = start_str\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context to block_size\n",
    "        context_crop = context if context.shape[1] <= block_size else context[:, -block_size:]\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = model(context_crop)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Sample from distribution\n",
    "        next_idx = mx.random.categorical(logits)[0]\n",
    "        \n",
    "        # Append to context\n",
    "        context = mx.concatenate([context, next_idx.reshape(1, 1)], axis=1)\n",
    "        generated += idx_to_char[int(next_idx)]\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Generate!\n",
    "seed = \"To be\"\n",
    "print(f\"Seed: '{seed}'\\n\")\n",
    "print(\"Generated text:\")\n",
    "print(\"=\"*50)\n",
    "generated = generate_text(model, seed, max_new_tokens=200, temperature=0.8)\n",
    "print(generated)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bb025",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have the architecture, we can train this model on our text corpus. But training a large model from scratch takes a lot of data and compute.\n",
    "\n",
    "In the next notebook, we will learn how to take a **pre-trained** giant model (like Llama 3) and fine-tune it using **LoRA**: **[07_Fine_Tuning_with_LoRA.ipynb](07_Fine_Tuning_with_LoRA.ipynb)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb459f82",
   "metadata": {},
   "source": [
    "## â“ FAQ\n",
    "\n",
    "**Q: How long does it take to train a \"real\" GPT?**\n",
    "A: Training GPT-3 took thousands of GPUs running for months. However, training a small \"NanoGPT\" on Shakespeare can be done on a MacBook in minutes to hours.\n",
    "\n",
    "**Q: Why \"Decoder-only\"?**\n",
    "A:\n",
    "*   **Encoder (BERT):** Good for understanding (Classification). Bi-directional attention.\n",
    "*   **Decoder (GPT):** Good for generation. Causal attention (can't see future).\n",
    "*   **Encoder-Decoder (T5/Bart):** Good for translation.\n",
    "\n",
    "**Q: What is \"LayerNorm\"?**\n",
    "A: It normalizes the inputs to have mean 0 and variance 1. This stabilizes training, allowing us to train much deeper networks without gradients exploding or vanishing.\n",
    "\n",
    "## ðŸ’­ Closing Thoughts\n",
    "\n",
    "**Scaling Laws**\n",
    "One of the most profound discoveries in AI is that Transformer performance is predictable. If you double the data and double the compute, the loss decreases by a predictable amount. This \"Scaling Law\" is what gave companies the confidence to invest billions in training larger and larger models.\n",
    "\n",
    "**Architectural Trade-off:**\n",
    "*   **Small Models:** Fast, cheap, run on edge devices. Good for specific tasks.\n",
    "*   **Large Models:** General purpose, \"emergent\" abilities (reasoning, coding), but expensive and slow.\n",
    "\n",
    "The future is likely a mix: Giant models in the cloud for reasoning, and small models (like this one!) on your device for privacy and speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
