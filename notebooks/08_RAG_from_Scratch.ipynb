{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f0f8eb",
   "metadata": {},
   "source": [
    "# üîç RAG from Scratch: Building Production Search Systems\n",
    "\n",
    "## What You'll Learn\n",
    "- **RAG Architecture**: Retrieval + Generation pipeline\n",
    "- **Vector Search**: Transform text into embeddings and find semantic matches\n",
    "- **System Design**: Scale from 7 docs to 100M docs\n",
    "- **Trade-offs**: RAG vs Fine-Tuning, Exact vs Approximate search\n",
    "\n",
    "## Why RAG?\n",
    "\n",
    "**The Problem with Static Models:**\n",
    "- GPT-4 doesn't know about your company's internal docs\n",
    "- Fine-tuning is expensive and makes the model \"stale\"\n",
    "\n",
    "**The RAG Solution:**\n",
    "1. **Store** knowledge in a searchable database (Vector DB)\n",
    "2. **Retrieve** relevant context when a query comes in\n",
    "3. **Generate** answer using LLM + retrieved context\n",
    "\n",
    "**Key Advantage**: Update the database anytime without retraining the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafffb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "import matplotlib.pyplot as plt\n",
    "from mlx_nlp_utils import print_device_info, load_rag_knowledge_base\n",
    "\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedef20b",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion & Chunking Strategy\n",
    "\n",
    "**Architectural Decision:** How do we split our text?\n",
    "- **Fixed-size Chunking:** Simple, fast. Risk: Cutting a sentence in half.\n",
    "- **Semantic Chunking:** Split by paragraph/topic. Better retrieval, harder to implement.\n",
    "- **Recursive Chunking:** Split by paragraph, then sentence, then word.\n",
    "\n",
    "For this demo, we will use **Paragraph-level Chunking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Knowledge Base (Simulating a Company Wiki)\n",
    "# We load from our synthetic data generator\n",
    "knowledge_base = load_rag_knowledge_base()\n",
    "\n",
    "print(f\"üìö Knowledge Base: {len(knowledge_base)} documents\")\n",
    "for i, doc in enumerate(knowledge_base[:3]):\n",
    "    print(f\"   Doc {i}: {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe20c1",
   "metadata": {},
   "source": [
    "## 2. Embeddings (The Vector Space)\n",
    "\n",
    "We need to convert text into vectors. In production, you'd use a model like `bert-base` or `nomic-embed-text`.\n",
    "\n",
    "**Trade-off:**\n",
    "- **Small Models (384 dim):** Fast search, less semantic nuance.\n",
    "- **Large Models (1024+ dim):** Better understanding, slower search, more RAM.\n",
    "\n",
    "For this tutorial, we will simulate embeddings to keep dependencies low, but the math is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a 4-dimensional embedding space for visualization purposes\n",
    "# In reality, this would be 768 or 1024 dimensions\n",
    "embedding_dim = 4\n",
    "\n",
    "# Mock embedding function (In production: use mlx-embeddings or sentence-transformers)\n",
    "def get_embedding(text):\n",
    "    # Deterministic random vector based on string hash for reproducibility\n",
    "    seed = sum([ord(c) for c in text])\n",
    "    mx.random.seed(seed)\n",
    "    vector = mx.random.normal((embedding_dim,))\n",
    "    # Normalize to unit length (Crucial for Cosine Similarity!)\n",
    "    return vector / mx.linalg.norm(vector)\n",
    "\n",
    "# Create Vector Database\n",
    "vector_db = []\n",
    "for doc in knowledge_base:\n",
    "    vector_db.append(get_embedding(doc))\n",
    "\n",
    "vector_db = mx.stack(vector_db)\n",
    "print(f\"üóÑÔ∏è Vector DB Shape: {vector_db.shape} (Docs, Dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b32b7b",
   "metadata": {},
   "source": [
    "## 3. Retrieval: Cosine Similarity\n",
    "\n",
    "We want to find the document vector $\\mathbf{d}$ that is closest to our query vector $\\mathbf{q}$.\n",
    "\n",
    "The standard metric is **Cosine Similarity**:\n",
    "$$ \\text{similarity} = \\cos(\\theta) = \\frac{\\mathbf{q} \\cdot \\mathbf{d}}{\\|\\mathbf{q}\\| \\|\\mathbf{d}\\|} $$\n",
    "\n",
    "Since we normalized our vectors (length = 1), this simplifies to just the **Dot Product**:\n",
    "$$ \\text{similarity} = \\mathbf{q} \\cdot \\mathbf{d} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f202e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    # 1. Embed query\n",
    "    query_vec = get_embedding(query)\n",
    "    \n",
    "    # 2. Compute scores (Dot Product)\n",
    "    # (Docs, Dim) @ (Dim,) -> (Docs,)\n",
    "    scores = vector_db @ query_vec\n",
    "    \n",
    "    # 3. Get top-k indices\n",
    "    # MLX doesn't have topk yet, so we use argsort\n",
    "    indices = mx.argsort(scores)[::-1][:k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices.tolist():\n",
    "        results.append((knowledge_base[idx], scores[idx].item()))\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Test Retrieval\n",
    "query = \"How does MLX handle memory?\"\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "\n",
    "hits = retrieve(query)\n",
    "for i, (doc, score) in enumerate(hits):\n",
    "    print(f\"Hit {i+1} (Score: {score:.3f}):\\n   \\\"{doc}\\\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb36cd",
   "metadata": {},
   "source": [
    "## 4. Generation (The \"G\" in RAG)\n",
    "\n",
    "Now we combine the retrieved context with the user query to prompt the LLM.\n",
    "\n",
    "**Architectural Consideration: Context Window**\n",
    "- If we retrieve too many documents, we exceed the context window (or pay huge API costs).\n",
    "- **Lost in the Middle:** LLMs tend to ignore information in the middle of a long context. Put the most relevant chunks at the start or end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b222e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_prompt(query, hits):\n",
    "    context_str = \"\\n\".join([f\"- {doc}\" for doc, score in hits])\n",
    "    \n",
    "    prompt = f\"\"\"<|user|>\n",
    "Answer the question based ONLY on the following context:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "rag_prompt = generate_rag_prompt(query, hits)\n",
    "print(\"üìù Final Prompt for LLM:\")\n",
    "print(\"=\"*40)\n",
    "print(rag_prompt)\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with actual use cases!\n",
    "print(\"\\nüß™ TESTING RAG SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "queries = [\n",
    "    \"What is MLX?\",\n",
    "    \"Why are transformers better than LSTMs?\",\n",
    "    \"How do I fine-tune without using too much memory?\",\n",
    "    \"What is the difference between fine-tuning and RAG?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    hits = retrieve(query, k=2)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(hits, 1):\n",
    "        print(f\"{i}. [Score: {score:.3f}] {doc[:80]}...\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292412c3",
   "metadata": {},
   "source": [
    "## 5. System Design: Scaling to 100 Million Docs\n",
    "\n",
    "In an interview, you will be asked: *\"This works for 7 sentences. How does it work for 100M?\"*\n",
    "\n",
    "### The Problem: Exact Search is $O(N)$\n",
    "Calculating cosine similarity against 100M vectors takes too long.\n",
    "\n",
    "### The Solution: ANN (Approximate Nearest Neighbors)\n",
    "We trade accuracy for speed using algorithms like **HNSW (Hierarchical Navigable Small World)** or **IVF (Inverted File Index)**.\n",
    "\n",
    "1.  **HNSW**: Builds a graph where nodes are vectors. Search navigates the graph greedily. $O(\\log N)$.\n",
    "2.  **Quantization**: Compress 32-bit floats to 8-bit integers (or binary) to fit index in RAM.\n",
    "\n",
    "### Hybrid Search\n",
    "Vector search is bad at exact keyword matching (e.g., searching for a specific SKU \"XJ-900\").\n",
    "**Best Practice:** Combine Vector Search (Semantic) + BM25 (Keyword) using **Reciprocal Rank Fusion (RRF)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4777417",
   "metadata": {},
   "source": [
    "## 8. Production Checklist\n",
    "\n",
    "Before deploying RAG to production, ensure:\n",
    "\n",
    "### ‚úÖ Data Quality\n",
    "- [ ] Chunk size optimized (128-512 tokens)\n",
    "- [ ] Metadata attached (source, timestamp)\n",
    "- [ ] Duplicates removed\n",
    "\n",
    "### ‚úÖ Search Quality\n",
    "- [ ] Threshold for \"no answer\" (e.g., score < 0.3)\n",
    "- [ ] Hybrid search implemented (Vector + BM25)\n",
    "- [ ] Re-ranking layer added\n",
    "\n",
    "### ‚úÖ Performance\n",
    "- [ ] HNSW index for > 100K docs\n",
    "- [ ] Quantization enabled (PQ or Scalar)\n",
    "- [ ] Caching for frequent queries\n",
    "\n",
    "### ‚úÖ Monitoring\n",
    "- [ ] Log retrieval scores\n",
    "- [ ] Track \"no answer\" rate\n",
    "- [ ] A/B test different chunking strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full RAG Pipeline Simulation\n",
    "def rag_pipeline(user_query, top_k=2):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline:\n",
    "    1. User asks question\n",
    "    2. Retrieve relevant docs\n",
    "    3. Build prompt with context\n",
    "    4. Generate answer with LLM\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç RAG Pipeline for: '{user_query}'\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Step 1: Retrieve\n",
    "    print(\"üìö RETRIEVAL PHASE:\")\n",
    "    hits = retrieve(user_query, k=top_k)\n",
    "    for i, (doc, score) in enumerate(hits, 1):\n",
    "        print(f\"   {i}. [{score:.2f}] {doc[:60]}...\")\n",
    "    \n",
    "    # Step 2: Build prompt\n",
    "    print(\"\\nüìù GENERATION PHASE:\")\n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc, _ in hits])\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful assistant. Answer the question using ONLY the provided context.\n",
    "If the answer is not in the context, say \"I don't have that information.\"\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "    print(\"   Prompt constructed with retrieved context\")\n",
    "    print(f\"   Prompt length: {len(prompt)} characters\")\n",
    "    \n",
    "    # Step 3: Generate (simulated)\n",
    "    print(\"\\nü§ñ LLM RESPONSE:\")\n",
    "    print(\"   [In production, this calls: generate(model, tokenizer, prompt)]\")\n",
    "    print(\"   Example output: 'Based on the context, MLX is an array framework...'\")\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test the full pipeline\n",
    "query = \"What makes MLX different from CUDA?\"\n",
    "final_prompt = rag_pipeline(query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ RAG Pipeline Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469e75c",
   "metadata": {},
   "source": [
    "## 7. Integration with LLM (Full RAG Pipeline)\n",
    "\n",
    "Let's see how this would connect to an actual LLM in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Case 1: Query doesn't match any documents\n",
    "print(\"üö® EDGE CASE: Out-of-Domain Query\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bad_query = \"How do I cook pasta?\"\n",
    "print(f\"Query: {bad_query}\\n\")\n",
    "\n",
    "hits = retrieve(bad_query, k=2)\n",
    "for i, (doc, score) in enumerate(hits, 1):\n",
    "    print(f\"{i}. [Score: {score:.3f}] {doc}\")\n",
    "\n",
    "print(\"\\nüí° Solution: Set a threshold (e.g., score < 0.3 ‚Üí 'I don't know')\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Edge Case 2: Multi-hop reasoning\n",
    "print(\"\\n\\nüö® EDGE CASE: Multi-Hop Question\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "complex_query = \"If I want to use MLX and need parallelization, should I use LSTMs?\"\n",
    "print(f\"Query: {complex_query}\\n\")\n",
    "\n",
    "hits = retrieve(complex_query, k=3)\n",
    "for i, (doc, score) in enumerate(hits, 1):\n",
    "    print(f\"{i}. [Score: {score:.3f}] {doc}\")\n",
    "\n",
    "print(\"\\nüí° Solution: Use LLM to decompose query into sub-questions\")\n",
    "print(\"   1. What is MLX? ‚Üí [retrieve]\")\n",
    "print(\"   2. Can LSTMs parallelize? ‚Üí [retrieve]\")\n",
    "print(\"   3. LLM synthesizes: 'Use Transformers, not LSTMs'\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c709a75",
   "metadata": {},
   "source": [
    "## 6. When RAG Fails: Edge Cases\n",
    "\n",
    "Understanding failure modes is critical for production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bf810",
   "metadata": {},
   "source": [
    "### Trade-off Table: Choosing Your Search Strategy\n",
    "\n",
    "| Scenario | Best Approach | Why |\n",
    "|----------|--------------|-----|\n",
    "| 1M documents, semantic search | HNSW (Faiss/Milvus) | Fast approximate search |\n",
    "| 100K documents, need 100% recall | Exact search (what we built) | Small enough for brute force |\n",
    "| Need exact SKU/ID matches | Hybrid (Vector + BM25) | Keyword search for IDs |\n",
    "| Real-time updates | Vector DB with incremental indexing | No rebuild needed |\n",
    "| Privacy-sensitive data | On-device MLX embeddings | No API calls |\n",
    "\n",
    "### Code Snippet: Production HNSW (Conceptual)\n",
    "\n",
    "```python\n",
    "# Using Faiss (Facebook AI Similarity Search)\n",
    "import faiss\n",
    "\n",
    "# Build index\n",
    "index = faiss.IndexHNSWFlat(embedding_dim, 32)  # 32 = connections per node\n",
    "index.add(vector_db_np)  # numpy array\n",
    "\n",
    "# Search\n",
    "D, I = index.search(query_vec_np, k=5)  # D=distances, I=indices\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb258499",
   "metadata": {},
   "source": [
    "## ‚ùì FAQ\n",
    "\n",
    "**Q: Vector Search vs. Keyword Search?**\n",
    "A:\n",
    "*   **Vector (Semantic):** Finds \"meaning\". Query \"dog\" matches \"puppy\". Good for concepts.\n",
    "*   **Keyword (Lexical):** Finds exact matches. Query \"Error 503\" matches \"Error 503\". Good for specific IDs/names.\n",
    "*   **Hybrid:** The best systems use both (Reciprocal Rank Fusion).\n",
    "\n",
    "**Q: How do I handle stale data?**\n",
    "A: That is the main advantage of RAG over Fine-Tuning. You just update the Vector Database (add/delete/update vectors). The LLM doesn't need to change.\n",
    "\n",
    "**Q: What is \"Re-ranking\"?**\n",
    "A: Vector search is fast but approximate. A common pattern is to retrieve 50 documents using vectors, then use a slower, more accurate \"Cross-Encoder\" model to re-rank the top 50 and pick the best 5 for the LLM.\n",
    "\n",
    "## üí≠ Closing Thoughts\n",
    "\n",
    "**The Future of Context Windows**\n",
    "As LLMs support larger contexts (1M+ tokens), do we still need RAG?\n",
    "*   **Yes:** For latency and cost. Reading 1M tokens takes time and money.\n",
    "*   **Yes:** For privacy. You don't want to send your entire database to the model for every query.\n",
    "\n",
    "**Architectural Evolution:**\n",
    "RAG is evolving from \"Static Retrieval\" to \"Agentic Retrieval\"‚Äîwhere the LLM decides *what* to search for, *when* to search, and *how* to filter the results. You are building the foundation for these autonomous agents."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
