{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce8ca1d",
   "metadata": {},
   "source": [
    "# üéì Fine-Tuning LLMs with LoRA on Apple Silicon\n",
    "\n",
    "## What You'll Learn\n",
    "- **LoRA (Low-Rank Adaptation)**: Train a 1B parameter model on a laptop\n",
    "- **Quantization**: Run models 4x smaller with 4-bit precision\n",
    "- **MLX-LM**: Apple's unified interface for LLMs\n",
    "- **Production Ready**: Chat templates and modern best practices (2025)\n",
    "\n",
    "## The Problem: Traditional Fine-Tuning is Expensive\n",
    "\n",
    "Training a 7B parameter model requires:\n",
    "- **Full Fine-Tuning**: 80GB+ VRAM, $10,000+ GPU\n",
    "- **LoRA**: 16GB RAM, Your MacBook (Free!)\n",
    "\n",
    "## The Solution: LoRA\n",
    "\n",
    "Instead of updating all 7 billion parameters, LoRA:\n",
    "1. **Freezes** the original model weights\n",
    "2. **Adds** small \"adapter\" matrices (rank-deficient)\n",
    "3. **Trains** only these adapters (<1% of parameters)\n",
    "\n",
    "Result: 99% of the performance, 1% of the memory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d57cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlx-lm in ./.venv/lib/python3.13/site-packages (0.28.3)\n",
      "Requirement already satisfied: mlx>=0.29.2 in ./.venv/lib/python3.13/site-packages (from mlx-lm) (0.29.4)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from mlx-lm) (2.3.5)\n",
      "Requirement already satisfied: transformers>=4.39.3 in ./.venv/lib/python3.13/site-packages (from mlx-lm) (4.57.1)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.13/site-packages (from mlx-lm) (6.33.1)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from mlx-lm) (6.0.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from mlx-lm) (3.1.6)\n",
      "Requirement already satisfied: mlx-metal==0.29.4 in ./.venv/lib/python3.13/site-packages (from mlx>=0.29.2->mlx-lm) (0.29.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->mlx-lm) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# Install mlx-lm if not already installed\n",
    "!pip install mlx-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37d1f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è  Hardware Acceleration Check:\n",
      "   Device: Device(gpu, 0)\n",
      "   ‚úÖ Using Apple Silicon GPU (Metal)\n",
      "   ‚ÑπÔ∏è  MLX automatically optimizes for the GPU's Unified Memory.\n",
      "   ‚ÑπÔ∏è  Note: While Apple Silicon has an NPU (Neural Engine), MLX primarily\n",
      "       uses the powerful GPU for general-purpose training tasks like LSTMs.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from mlx_lm import load, generate\n",
    "from mlx_nlp_utils import print_device_info\n",
    "\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d5b6a",
   "metadata": {},
   "source": [
    "## 1. Load a Pre-Trained Model\n",
    "\n",
    "We will use a small but capable model like **Mistral-7B** or **TinyLlama** (depending on your RAM). MLX handles the downloading automatically from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cf5d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mlx-community/Llama-3.2-1B-Instruct-4bit...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61cad68d43e4f77afc03fff25074484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "# Using Llama-3.2-1B-Instruct (State of the art small model as of late 2024/2025)\n",
    "model_name = \"mlx-community/Llama-3.2-1B-Instruct-4bit\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7dea0",
   "metadata": {},
   "source": [
    "## 2. Test Base Model\n",
    "\n",
    "Let's see how it performs *before* fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ff2230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 19 Nov 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "====================\n",
      "==========\n",
      "TheThe capital of capital of France is France is Paris. Paris.\n",
      "==========\n",
      "Prompt: 42 tokens, 260.619 tokens-per-sec\n",
      "Generation: 8 tokens, 146.494 tokens-per-sec\n",
      "Peak memory: 1.457 GB\n",
      "\n",
      "==========\n",
      "Prompt: 42 tokens, 260.619 tokens-per-sec\n",
      "Generation: 8 tokens, 146.494 tokens-per-sec\n",
      "Peak memory: 1.457 GB\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer's chat template (Modern Best Practice)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"Formatted Prompt:\\n{prompt}\\n{'='*20}\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1c03b",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Fine-Tuning\n",
    "\n",
    "We need data in a specific format (JSONL) for `mlx-lm`. Let's convert our Intent/Sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42c21e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7 examples to data/train.jsonl\n",
      "Saved 2 examples to data/valid.jsonl\n",
      "\n",
      "‚úÖ Data prepared for MLX LoRA!\n",
      "   Format: JSONL (Chat format)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load our existing Intent Classification data\n",
    "data_path = Path('../data/intent_samples/data.json')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"‚ö†Ô∏è Data not found. Please run: python ../scripts/download_datasets.py --samples\")\n",
    "else:\n",
    "    with open(data_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    texts = raw_data['texts']\n",
    "    labels = raw_data['labels']\n",
    "    \n",
    "    # 2. Convert to Chat Format (JSONL)\n",
    "    # We want the model to learn to classify intents.\n",
    "    # User: \"Turn on the lights\" -> Assistant: \"intent: command\"\n",
    "    \n",
    "    chat_data = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{text}'\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"intent: {label}\"}\n",
    "            ]\n",
    "        }\n",
    "        chat_data.append(entry)\n",
    "    \n",
    "    # 3. Save as train.jsonl and valid.jsonl\n",
    "    # Split 80/20\n",
    "    split_idx = int(len(chat_data) * 0.8)\n",
    "    train_data = chat_data[:split_idx]\n",
    "    valid_data = chat_data[split_idx:]\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    Path('data').mkdir(exist_ok=True)\n",
    "    \n",
    "    def save_jsonl(data, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "        print(f\"Saved {len(data)} examples to {filename}\")\n",
    "\n",
    "    save_jsonl(train_data, 'data/train.jsonl')\n",
    "    save_jsonl(valid_data, 'data/valid.jsonl')\n",
    "    \n",
    "    print(\"\\n‚úÖ Data prepared for MLX LoRA!\")\n",
    "    print(\"   Format: JSONL (Chat format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2373b",
   "metadata": {},
   "source": [
    "## 4. Run Fine-Tuning (LoRA)\n",
    "\n",
    "We can use the `mlx_lm.lora` command line tool or API to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a11e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Command (for reference):\n",
      "============================================================\n",
      "cd notebooks && python -m mlx_lm.lora \\\n",
      "    --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\n",
      "    --train \\\n",
      "    --data data \\\n",
      "    --batch-size 4 \\\n",
      "    --lora-layers 16 \\\n",
      "    --iters 600 \\\n",
      "    --learning-rate 1e-4 \\\n",
      "    --adapter-path ./adapters\n",
      "============================================================\n",
      "\n",
      "üí° Recommendation: Run the next cell to train in the notebook!\n",
      "   (It handles paths automatically)\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "# We use the CLI tool provided by mlx-lm\n",
    "# --batch-size 4: Fits easily on 8GB/16GB RAM\n",
    "# --lora-layers 16: Target more layers for better quality\n",
    "# --iters 600: Enough for a small dataset\n",
    "\n",
    "print(\"üìã Training Command (for reference):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cmd = \"\"\"cd notebooks && python -m mlx_lm.lora \\\\\n",
    "    --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\\\n",
    "    --train \\\\\n",
    "    --data data \\\\\n",
    "    --batch-size 4 \\\\\n",
    "    --lora-layers 16 \\\\\n",
    "    --iters 600 \\\\\n",
    "    --learning-rate 1e-4 \\\\\n",
    "    --adapter-path ./adapters\"\"\"\n",
    "\n",
    "print(cmd)\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Recommendation: Run the next cell to train in the notebook!\")\n",
    "print(\"   (It handles paths automatically)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Project Root: .\n",
      "üêç Venv Python: ./.venv/bin/python\n",
      "\n",
      "üöÄ Starting LoRA Training using .venv...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Fetching 6 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 117050.34it/s]\n",
      "Fetching 6 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 117050.34it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.456% (5.636M/1235.814M)\n",
      "Starting training..., iters: 600\n",
      "Calculating loss...:   0%|                                | 0/1 [00:00<?, ?it/s]Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.456% (5.636M/1235.814M)\n",
      "Starting training..., iters: 600\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Iter 1: Val loss 6.362, Val took 0.583s\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Iter 1: Val loss 6.362, Val took 0.583s\n",
      "Iter 10: Train loss 1.455, Learning Rate 1.000e-04, It/sec 2.261, Tokens/sec 237.835, Trained Tokens 1052, Peak mem 1.370 GB\n",
      "Iter 10: Train loss 1.455, Learning Rate 1.000e-04, It/sec 2.261, Tokens/sec 237.835, Trained Tokens 1052, Peak mem 1.370 GB\n",
      "Iter 20: Train loss 0.350, Learning Rate 1.000e-04, It/sec 3.193, Tokens/sec 334.947, Trained Tokens 2101, Peak mem 1.370 GB\n",
      "Iter 20: Train loss 0.350, Learning Rate 1.000e-04, It/sec 3.193, Tokens/sec 334.947, Trained Tokens 2101, Peak mem 1.370 GB\n",
      "Iter 30: Train loss 0.400, Learning Rate 1.000e-04, It/sec 3.179, Tokens/sec 333.464, Trained Tokens 3150, Peak mem 1.370 GB\n",
      "Iter 30: Train loss 0.400, Learning Rate 1.000e-04, It/sec 3.179, Tokens/sec 333.464, Trained Tokens 3150, Peak mem 1.370 GB\n",
      "Iter 40: Train loss 0.089, Learning Rate 1.000e-04, It/sec 3.182, Tokens/sec 334.787, Trained Tokens 4202, Peak mem 1.370 GB\n",
      "Iter 40: Train loss 0.089, Learning Rate 1.000e-04, It/sec 3.182, Tokens/sec 334.787, Trained Tokens 4202, Peak mem 1.370 GB\n",
      "Iter 50: Train loss 0.041, Learning Rate 1.000e-04, It/sec 3.193, Tokens/sec 334.919, Trained Tokens 5251, Peak mem 1.370 GB\n",
      "Iter 50: Train loss 0.041, Learning Rate 1.000e-04, It/sec 3.193, Tokens/sec 334.919, Trained Tokens 5251, Peak mem 1.370 GB\n",
      "Iter 60: Train loss 0.077, Learning Rate 1.000e-04, It/sec 3.121, Tokens/sec 327.415, Trained Tokens 6300, Peak mem 1.370 GB\n",
      "Iter 60: Train loss 0.077, Learning Rate 1.000e-04, It/sec 3.121, Tokens/sec 327.415, Trained Tokens 6300, Peak mem 1.370 GB\n",
      "Iter 70: Train loss 0.083, Learning Rate 1.000e-04, It/sec 2.997, Tokens/sec 314.428, Trained Tokens 7349, Peak mem 1.370 GB\n",
      "Iter 70: Train loss 0.083, Learning Rate 1.000e-04, It/sec 2.997, Tokens/sec 314.428, Trained Tokens 7349, Peak mem 1.370 GB\n",
      "Iter 80: Train loss 0.063, Learning Rate 1.000e-04, It/sec 3.062, Tokens/sec 322.163, Trained Tokens 8401, Peak mem 1.370 GB\n",
      "Iter 80: Train loss 0.063, Learning Rate 1.000e-04, It/sec 3.062, Tokens/sec 322.163, Trained Tokens 8401, Peak mem 1.370 GB\n",
      "Iter 90: Train loss 0.043, Learning Rate 1.000e-04, It/sec 3.155, Tokens/sec 330.950, Trained Tokens 9450, Peak mem 1.370 GB\n",
      "Iter 90: Train loss 0.043, Learning Rate 1.000e-04, It/sec 3.155, Tokens/sec 330.950, Trained Tokens 9450, Peak mem 1.370 GB\n",
      "Iter 100: Train loss 0.039, Learning Rate 1.000e-04, It/sec 3.137, Tokens/sec 329.116, Trained Tokens 10499, Peak mem 1.370 GB\n",
      "Iter 100: Train loss 0.039, Learning Rate 1.000e-04, It/sec 3.137, Tokens/sec 329.116, Trained Tokens 10499, Peak mem 1.370 GB\n",
      "Iter 100: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000100_adapters.safetensors.\n",
      "Iter 100: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.038, Learning Rate 1.000e-04, It/sec 3.121, Tokens/sec 328.279, Trained Tokens 11551, Peak mem 1.393 GB\n",
      "Iter 110: Train loss 0.038, Learning Rate 1.000e-04, It/sec 3.121, Tokens/sec 328.279, Trained Tokens 11551, Peak mem 1.393 GB\n",
      "Iter 120: Train loss 0.038, Learning Rate 1.000e-04, It/sec 3.142, Tokens/sec 329.575, Trained Tokens 12600, Peak mem 1.393 GB\n",
      "Iter 120: Train loss 0.038, Learning Rate 1.000e-04, It/sec 3.142, Tokens/sec 329.575, Trained Tokens 12600, Peak mem 1.393 GB\n",
      "Iter 130: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.183, Tokens/sec 333.912, Trained Tokens 13649, Peak mem 1.393 GB\n",
      "Iter 130: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.183, Tokens/sec 333.912, Trained Tokens 13649, Peak mem 1.393 GB\n",
      "Iter 140: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.164, Tokens/sec 332.855, Trained Tokens 14701, Peak mem 1.393 GB\n",
      "Iter 140: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.164, Tokens/sec 332.855, Trained Tokens 14701, Peak mem 1.393 GB\n",
      "Iter 150: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.189, Tokens/sec 334.511, Trained Tokens 15750, Peak mem 1.393 GB\n",
      "Iter 150: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.189, Tokens/sec 334.511, Trained Tokens 15750, Peak mem 1.393 GB\n",
      "Iter 160: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.190, Tokens/sec 334.618, Trained Tokens 16799, Peak mem 1.393 GB\n",
      "Iter 160: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.190, Tokens/sec 334.618, Trained Tokens 16799, Peak mem 1.393 GB\n",
      "Iter 170: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.149, Tokens/sec 331.224, Trained Tokens 17851, Peak mem 1.393 GB\n",
      "Iter 170: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.149, Tokens/sec 331.224, Trained Tokens 17851, Peak mem 1.393 GB\n",
      "Iter 180: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.195, Tokens/sec 335.182, Trained Tokens 18900, Peak mem 1.393 GB\n",
      "Iter 180: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.195, Tokens/sec 335.182, Trained Tokens 18900, Peak mem 1.393 GB\n",
      "Iter 190: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.171, Tokens/sec 332.635, Trained Tokens 19949, Peak mem 1.393 GB\n",
      "Iter 190: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.171, Tokens/sec 332.635, Trained Tokens 19949, Peak mem 1.393 GB\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.91it/s]\n",
      "Iter 200: Val loss 0.565, Val took 0.152s\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.91it/s]\n",
      "Iter 200: Val loss 0.565, Val took 0.152s\n",
      "Iter 200: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 332.200, Trained Tokens 20998, Peak mem 1.393 GB\n",
      "Iter 200: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 332.200, Trained Tokens 20998, Peak mem 1.393 GB\n",
      "Iter 200: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000200_adapters.safetensors.\n",
      "Iter 200: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 333.151, Trained Tokens 22050, Peak mem 1.393 GB\n",
      "Iter 210: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 333.151, Trained Tokens 22050, Peak mem 1.393 GB\n",
      "Iter 220: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.177, Tokens/sec 333.288, Trained Tokens 23099, Peak mem 1.393 GB\n",
      "Iter 220: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.177, Tokens/sec 333.288, Trained Tokens 23099, Peak mem 1.393 GB\n",
      "Iter 230: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.139, Tokens/sec 329.306, Trained Tokens 24148, Peak mem 1.393 GB\n",
      "Iter 230: Train loss 0.037, Learning Rate 1.000e-04, It/sec 3.139, Tokens/sec 329.306, Trained Tokens 24148, Peak mem 1.393 GB\n",
      "Iter 240: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.182, Tokens/sec 334.722, Trained Tokens 25200, Peak mem 1.393 GB\n",
      "Iter 240: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.182, Tokens/sec 334.722, Trained Tokens 25200, Peak mem 1.393 GB\n",
      "Iter 250: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.204, Tokens/sec 336.115, Trained Tokens 26249, Peak mem 1.393 GB\n",
      "Iter 250: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.204, Tokens/sec 336.115, Trained Tokens 26249, Peak mem 1.393 GB\n",
      "Iter 260: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.166, Tokens/sec 333.013, Trained Tokens 27301, Peak mem 1.393 GB\n",
      "Iter 260: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.166, Tokens/sec 333.013, Trained Tokens 27301, Peak mem 1.393 GB\n",
      "Iter 270: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.163, Tokens/sec 331.782, Trained Tokens 28350, Peak mem 1.393 GB\n",
      "Iter 270: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.163, Tokens/sec 331.782, Trained Tokens 28350, Peak mem 1.393 GB\n",
      "Iter 280: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.170, Tokens/sec 333.469, Trained Tokens 29402, Peak mem 1.393 GB\n",
      "Iter 280: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.170, Tokens/sec 333.469, Trained Tokens 29402, Peak mem 1.393 GB\n",
      "Iter 290: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.161, Tokens/sec 331.595, Trained Tokens 30451, Peak mem 1.393 GB\n",
      "Iter 290: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.161, Tokens/sec 331.595, Trained Tokens 30451, Peak mem 1.393 GB\n",
      "Iter 300: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.223, Tokens/sec 338.069, Trained Tokens 31500, Peak mem 1.393 GB\n",
      "Iter 300: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.223, Tokens/sec 338.069, Trained Tokens 31500, Peak mem 1.393 GB\n",
      "Iter 300: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000300_adapters.safetensors.\n",
      "Iter 300: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.190, Tokens/sec 335.554, Trained Tokens 32552, Peak mem 1.393 GB\n",
      "Iter 310: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.190, Tokens/sec 335.554, Trained Tokens 32552, Peak mem 1.393 GB\n",
      "Iter 320: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.212, Tokens/sec 336.930, Trained Tokens 33601, Peak mem 1.393 GB\n",
      "Iter 320: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.212, Tokens/sec 336.930, Trained Tokens 33601, Peak mem 1.393 GB\n",
      "Iter 330: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.213, Tokens/sec 337.085, Trained Tokens 34650, Peak mem 1.393 GB\n",
      "Iter 330: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.213, Tokens/sec 337.085, Trained Tokens 34650, Peak mem 1.393 GB\n",
      "Iter 340: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.165, Tokens/sec 332.928, Trained Tokens 35702, Peak mem 1.393 GB\n",
      "Iter 340: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.165, Tokens/sec 332.928, Trained Tokens 35702, Peak mem 1.393 GB\n",
      "Iter 350: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.173, Tokens/sec 332.885, Trained Tokens 36751, Peak mem 1.393 GB\n",
      "Iter 350: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.173, Tokens/sec 332.885, Trained Tokens 36751, Peak mem 1.393 GB\n",
      "Iter 360: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.186, Tokens/sec 334.261, Trained Tokens 37800, Peak mem 1.393 GB\n",
      "Iter 360: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.186, Tokens/sec 334.261, Trained Tokens 37800, Peak mem 1.393 GB\n",
      "Iter 370: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 332.227, Trained Tokens 38849, Peak mem 1.393 GB\n",
      "Iter 370: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 332.227, Trained Tokens 38849, Peak mem 1.393 GB\n",
      "Iter 380: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.166, Tokens/sec 333.060, Trained Tokens 39901, Peak mem 1.393 GB\n",
      "Iter 380: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.166, Tokens/sec 333.060, Trained Tokens 39901, Peak mem 1.393 GB\n",
      "Iter 390: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.181, Tokens/sec 333.663, Trained Tokens 40950, Peak mem 1.393 GB\n",
      "Iter 390: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.181, Tokens/sec 333.663, Trained Tokens 40950, Peak mem 1.393 GB\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Iter 400: Val loss 0.566, Val took 0.149s\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Iter 400: Val loss 0.566, Val took 0.149s\n",
      "Iter 400: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.175, Tokens/sec 333.012, Trained Tokens 41999, Peak mem 1.393 GB\n",
      "Iter 400: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.175, Tokens/sec 333.012, Trained Tokens 41999, Peak mem 1.393 GB\n",
      "Iter 400: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000400_adapters.safetensors.\n",
      "Iter 400: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.176, Tokens/sec 334.069, Trained Tokens 43051, Peak mem 1.393 GB\n",
      "Iter 410: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.176, Tokens/sec 334.069, Trained Tokens 43051, Peak mem 1.393 GB\n",
      "Iter 420: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.198, Tokens/sec 335.496, Trained Tokens 44100, Peak mem 1.393 GB\n",
      "Iter 420: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.198, Tokens/sec 335.496, Trained Tokens 44100, Peak mem 1.393 GB\n",
      "Iter 430: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.153, Tokens/sec 331.699, Trained Tokens 45152, Peak mem 1.393 GB\n",
      "Iter 430: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.153, Tokens/sec 331.699, Trained Tokens 45152, Peak mem 1.393 GB\n",
      "Iter 440: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.169, Tokens/sec 332.432, Trained Tokens 46201, Peak mem 1.393 GB\n",
      "Iter 440: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.169, Tokens/sec 332.432, Trained Tokens 46201, Peak mem 1.393 GB\n",
      "Iter 450: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 332.176, Trained Tokens 47250, Peak mem 1.393 GB\n",
      "Iter 450: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 332.176, Trained Tokens 47250, Peak mem 1.393 GB\n",
      "Iter 460: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 333.191, Trained Tokens 48302, Peak mem 1.393 GB\n",
      "Iter 460: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.167, Tokens/sec 333.191, Trained Tokens 48302, Peak mem 1.393 GB\n",
      "Iter 470: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.159, Tokens/sec 331.360, Trained Tokens 49351, Peak mem 1.393 GB\n",
      "Iter 470: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.159, Tokens/sec 331.360, Trained Tokens 49351, Peak mem 1.393 GB\n",
      "Iter 480: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.147, Tokens/sec 330.080, Trained Tokens 50400, Peak mem 1.393 GB\n",
      "Iter 480: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.147, Tokens/sec 330.080, Trained Tokens 50400, Peak mem 1.393 GB\n",
      "Iter 490: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.163, Tokens/sec 332.744, Trained Tokens 51452, Peak mem 1.393 GB\n",
      "Iter 490: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.163, Tokens/sec 332.744, Trained Tokens 51452, Peak mem 1.393 GB\n",
      "Iter 500: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.148, Tokens/sec 330.264, Trained Tokens 52501, Peak mem 1.393 GB\n",
      "Iter 500: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000500_adapters.safetensors.\n",
      "Iter 500: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.148, Tokens/sec 330.264, Trained Tokens 52501, Peak mem 1.393 GB\n",
      "Iter 500: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.179, Tokens/sec 333.431, Trained Tokens 53550, Peak mem 1.393 GB\n",
      "Iter 510: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.179, Tokens/sec 333.431, Trained Tokens 53550, Peak mem 1.393 GB\n",
      "Iter 520: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.165, Tokens/sec 332.915, Trained Tokens 54602, Peak mem 1.393 GB\n",
      "Iter 520: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.165, Tokens/sec 332.915, Trained Tokens 54602, Peak mem 1.393 GB\n",
      "Iter 530: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.123, Tokens/sec 327.576, Trained Tokens 55651, Peak mem 1.393 GB\n",
      "Iter 530: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.123, Tokens/sec 327.576, Trained Tokens 55651, Peak mem 1.393 GB\n",
      "Iter 540: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.121, Tokens/sec 327.371, Trained Tokens 56700, Peak mem 1.393 GB\n",
      "Iter 540: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.121, Tokens/sec 327.371, Trained Tokens 56700, Peak mem 1.393 GB\n",
      "Iter 550: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.111, Tokens/sec 326.312, Trained Tokens 57749, Peak mem 1.393 GB\n",
      "Iter 550: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.111, Tokens/sec 326.312, Trained Tokens 57749, Peak mem 1.393 GB\n",
      "Iter 560: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.132, Tokens/sec 328.581, Trained Tokens 58798, Peak mem 1.393 GB\n",
      "Iter 560: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.132, Tokens/sec 328.581, Trained Tokens 58798, Peak mem 1.393 GB\n",
      "Iter 570: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.131, Tokens/sec 329.400, Trained Tokens 59850, Peak mem 1.393 GB\n",
      "Iter 570: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.131, Tokens/sec 329.400, Trained Tokens 59850, Peak mem 1.393 GB\n",
      "Iter 580: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.139, Tokens/sec 329.327, Trained Tokens 60899, Peak mem 1.393 GB\n",
      "Iter 580: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.139, Tokens/sec 329.327, Trained Tokens 60899, Peak mem 1.393 GB\n",
      "Iter 590: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.120, Tokens/sec 327.272, Trained Tokens 61948, Peak mem 1.393 GB\n",
      "Iter 590: Train loss 0.035, Learning Rate 1.000e-04, It/sec 3.120, Tokens/sec 327.272, Trained Tokens 61948, Peak mem 1.393 GB\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.77it/s]\n",
      "Iter 600: Val loss 0.578, Val took 0.153s\n",
      "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.77it/s]\n",
      "Iter 600: Val loss 0.578, Val took 0.153s\n",
      "Iter 600: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.131, Tokens/sec 329.429, Trained Tokens 63000, Peak mem 1.393 GB\n",
      "Iter 600: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000600_adapters.safetensors.\n",
      "Saved final weights to ./notebooks/adapters/adapters.safetensors.\n",
      "Iter 600: Train loss 0.036, Learning Rate 1.000e-04, It/sec 3.131, Tokens/sec 329.429, Trained Tokens 63000, Peak mem 1.393 GB\n",
      "Iter 600: Saved adapter weights to ./notebooks/adapters/adapters.safetensors and ./notebooks/adapters/0000600_adapters.safetensors.\n",
      "Saved final weights to ./notebooks/adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "# Run LoRA Training (5-10 minutes on M1/M2/M3)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ Starting LoRA Fine-Tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Setup Paths\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Find project root and working directory\n",
    "if (current_dir / \"notebooks\").exists():\n",
    "    project_root = current_dir\n",
    "    working_dir = current_dir / \"notebooks\"\n",
    "elif current_dir.name == \"notebooks\":\n",
    "    project_root = current_dir.parent\n",
    "    working_dir = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "    working_dir = current_dir\n",
    "\n",
    "data_dir = working_dir / \"data\"\n",
    "adapter_dir = working_dir / \"adapters\"\n",
    "\n",
    "print(f\"üìç Project Root: {project_root}\")\n",
    "print(f\"üìÇ Data Dir: {data_dir}\")\n",
    "print(f\"üíæ Adapter Dir: {adapter_dir}\")\n",
    "\n",
    "# 2. Verify Data Exists\n",
    "if not (data_dir / \"train.jsonl\").exists():\n",
    "    print(\"\\n‚ùå ERROR: 'train.jsonl' not found in data directory.\")\n",
    "    print(\"   Please run the 'Prepare Data' cell above first!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data found. Starting training...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 3. Run Training Command\n",
    "    # Using system python (assumes mlx-lm is installed in current environment)\n",
    "    !python -m mlx_lm.lora \\\n",
    "        --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\n",
    "        --train \\\n",
    "        --data \"{data_dir}\" \\\n",
    "        --batch-size 4 \\\n",
    "        --lora-layers 16 \\\n",
    "        --iters 100 \\\n",
    "        --learning-rate 1e-4 \\\n",
    "        --adapter-path \"{adapter_dir}\"\n",
    "    \n",
    "    print(\"\\n‚úÖ Training Complete!\")\n",
    "    print(f\"   Adapters saved to: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafae00",
   "metadata": {},
   "source": [
    "### What the Training Does\n",
    "\n",
    "The LoRA training will:\n",
    "1. Download the base model (if not cached)\n",
    "2. Load your training/validation data\n",
    "3. Add LoRA adapters to attention layers\n",
    "4. Train for 600 iterations (~5-10 minutes on M1/M2)\n",
    "5. Save the adapters to `./adapters`\n",
    "\n",
    "**Note**: This is a demo. For production:\n",
    "- Use more data (100+ examples minimum)\n",
    "- Train longer (1000-5000 iterations)\n",
    "- Tune hyperparameters (learning rate, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253bf00",
   "metadata": {},
   "source": [
    "## 5. Inference with Fine-Tuned Model\n",
    "\n",
    "After training completes, we can load the base model + adapters and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6dbfb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading fine-tuned model (base + LoRA adapters)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b9e721d0b846099397f939d7aba5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned model loaded!\n",
      "\n",
      "Test Query: 'Turn off the lights'\n",
      "============================================================\n",
      "\n",
      "üì¶ BASE MODEL:\n",
      "The intent of this text is to instruct or guide someone to perform a specific action, which in this case is to turn off the lights. It is a direct and clear command, likely from a household or home maintenance context.\n",
      "\n",
      "üéØ FINE-TUNED MODEL:\n",
      "The intent of this text is to instruct or guide someone to perform a specific action, which in this case is to turn off the lights. It is a direct and clear command, likely from a household or home maintenance context.\n",
      "\n",
      "üéØ FINE-TUNED MODEL:\n",
      "intent: question\n",
      "============================================================\n",
      "\n",
      "üí° Expected Improvement:\n",
      "   - Base model: Generic response or incorrect intent\n",
      "   - Fine-tuned: Correctly identifies 'intent: command'\n",
      "intent: question\n",
      "============================================================\n",
      "\n",
      "üí° Expected Improvement:\n",
      "   - Base model: Generic response or incorrect intent\n",
      "   - Fine-tuned: Correctly identifies 'intent: command'\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model (base + adapters) if available\n",
    "from pathlib import Path\n",
    "\n",
    "adapter_path = Path(\"./adapters\")\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(\"‚úÖ Loading fine-tuned model (base + LoRA adapters)...\")\n",
    "    model_ft, tokenizer_ft = load(model_name, adapter_path=str(adapter_path))\n",
    "    print(\"‚úÖ Fine-tuned model loaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No adapters found. Will use base model only.\")\n",
    "    print(\"   (Run the training command first to create adapters)\")\n",
    "    model_ft, tokenizer_ft = model, tokenizer\n",
    "\n",
    "# Side-by-side comparison\n",
    "test_query = \"Turn off the lights\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{test_query}'\"}\n",
    "]\n",
    "prompt_test = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüì¶ BASE MODEL:\")\n",
    "response_base = generate(model, tokenizer, prompt=prompt_test, max_tokens=50, verbose=False)\n",
    "print(response_base)\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(\"\\nüéØ FINE-TUNED MODEL:\")\n",
    "    response_ft = generate(model_ft, tokenizer_ft, prompt=prompt_test, max_tokens=50, verbose=False)\n",
    "    print(response_ft)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Fine-tuned model not available (run training first)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Expected Improvement:\")\n",
    "print(\"   - Base model: Generic response or incorrect intent\")\n",
    "print(\"   - Fine-tuned: Correctly identifies 'intent: command'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f046f",
   "metadata": {},
   "source": [
    "## 6. Compare: Base vs Fine-Tuned\n",
    "\n",
    "Let's see the difference between the base model and our fine-tuned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a3a3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing Fine-Tuned Model\n",
      "==================================================\n",
      "\n",
      "üìù Input: 'Turn on the light'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'What time is it'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'Hello there'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "üìù Input: 'Set a timer for 5 minutes'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'Hello there'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "üìù Input: 'Set a timer for 5 minutes'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'How are you doing'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "==================================================\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'How are you doing'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on intent classification\n",
    "if not adapter_path.exists():\n",
    "    print(\"‚ö†Ô∏è  No fine-tuned model available. Run training first.\")\n",
    "    print(\"   Using base model for testing.\")\n",
    "\n",
    "test_cases = [\n",
    "    \"Turn on the light\",\n",
    "    \"What time is it\",\n",
    "    \"Hello there\",\n",
    "    \"Set a timer for 5 minutes\",\n",
    "    \"How are you doing\"\n",
    "]\n",
    "\n",
    "model_to_test = model_ft if adapter_path.exists() else model\n",
    "tokenizer_to_test = tokenizer_ft if adapter_path.exists() else tokenizer\n",
    "model_type = \"Fine-Tuned\" if adapter_path.exists() else \"Base\"\n",
    "\n",
    "print(f\"\\nüß™ Testing {model_type} Model\\n\" + \"=\"*50)\n",
    "\n",
    "for test_text in test_cases:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{test_text}'\"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer_to_test.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    print(f\"\\nüìù Input: '{test_text}'\")\n",
    "    response = generate(model_to_test, tokenizer_to_test, prompt=prompt, max_tokens=50, verbose=False)\n",
    "    print(f\"ü§ñ Output: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cd30e",
   "metadata": {},
   "source": [
    "## Further Experiments: Optimizing LoRA\n",
    "\n",
    "Fine-tuning is an art as much as a science. Here are some experiments to deepen your understanding.\n",
    "\n",
    "### Experiment: Finding the Optimal Rank\n",
    "We used `lora_rank=8`. What happens if we change it?\n",
    "- **Rank 4:** Faster, less memory, but might not learn complex patterns.\n",
    "- **Rank 32:** Slower, more memory, potentially better quality.\n",
    "- **Rank 64+:** Diminishing returns, high risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Find optimal LoRA rank\n",
    "\n",
    "def test_lora_ranks():\n",
    "    \"\"\"Compare different LoRA ranks on same task\"\"\"\n",
    "    print(\"Testing LoRA Ranks...\")\n",
    "    \n",
    "    ranks = [4, 8, 16, 32]\n",
    "    \n",
    "    # In a real experiment, we would loop through these ranks, \n",
    "    # train a model for each, and evaluate on a validation set.\n",
    "    \n",
    "    for rank in ranks:\n",
    "        print(f\"Training with Rank {rank}...\")\n",
    "        # train_lora(rank=rank)\n",
    "        # score = evaluate()\n",
    "        # print(f\"Rank {rank} Score: {score}\")\n",
    "        pass\n",
    "        \n",
    "    print(\"General Rule: Rank 8 or 16 is usually the sweet spot for 7B models.\")\n",
    "\n",
    "# test_lora_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7751",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "You have now completed the journey from simple RNNs to fine-tuning modern LLMs on Apple Silicon!\n",
    "\n",
    "**What's Next?**\n",
    "- Build a RAG (Retrieval Augmented Generation) system.\n",
    "- Deploy your fine-tuned model as an API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c62467",
   "metadata": {},
   "source": [
    "## ‚ùì FAQ\n",
    "\n",
    "**Q: LoRA vs. Full Fine-Tuning?**\n",
    "A:\n",
    "*   **Full Fine-Tuning:** Updates all weights. Requires massive VRAM (e.g., 80GB+ for 7B model).\n",
    "*   **LoRA:** Updates <1% of weights. Runs on consumer hardware (e.g., 16GB MacBook). Performance is often 98-99% of full fine-tuning.\n",
    "\n",
    "**Q: Can I fine-tune on my own emails?**\n",
    "A: Yes! Just format them as JSONL: `{\"messages\": [{\"role\": \"user\", \"content\": \"Subject: Meeting\"}, {\"role\": \"assistant\", \"content\": \"Hi team...\"}]}`.\n",
    "\n",
    "**Q: What is \"Quantization\" (4-bit)?**\n",
    "A: It reduces the precision of weights from 16-bit (Float16) to 4-bit integers. This cuts memory usage by 4x with minimal loss in quality, allowing you to run a 7B model on a laptop.\n",
    "\n",
    "## üí≠ Closing Thoughts\n",
    "\n",
    "**The Commoditization of Intelligence**\n",
    "We are entering an era where \"Intelligence\" is a downloadable package.\n",
    "*   **2020:** Only OpenAI had GPT-3.\n",
    "*   **2025:** You can run a model nearly as smart as GPT-3 on your laptop, fine-tuned on your private data, with no internet connection.\n",
    "\n",
    "**Architectural Shift:**\n",
    "The role of the AI Engineer is shifting from \"Designing Architectures\" (building LSTMs) to \"Data Curation\" (preparing high-quality datasets for fine-tuning). The model is a solved problem; the data is your competitive advantage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
