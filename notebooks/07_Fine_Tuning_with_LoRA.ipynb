{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce8ca1d",
   "metadata": {},
   "source": [
    "# üéì Fine-Tuning LLMs with LoRA on Apple Silicon\n",
    "\n",
    "## What You'll Learn\n",
    "- **LoRA (Low-Rank Adaptation)**: Train a 1B parameter model on a laptop\n",
    "- **Quantization**: Run models 4x smaller with 4-bit precision\n",
    "- **MLX-LM**: Apple's unified interface for LLMs\n",
    "- **Production Ready**: Chat templates and modern best practices (2025)\n",
    "\n",
    "## The Problem: Traditional Fine-Tuning is Expensive\n",
    "\n",
    "Training a 7B parameter model requires:\n",
    "- **Full Fine-Tuning**: 80GB+ VRAM, $10,000+ GPU\n",
    "- **LoRA**: 16GB RAM, Your MacBook (Free!)\n",
    "\n",
    "## The Solution: LoRA\n",
    "\n",
    "Instead of updating all 7 billion parameters, LoRA:\n",
    "1. **Freezes** the original model weights\n",
    "2. **Adds** small \"adapter\" matrices (rank-deficient)\n",
    "3. **Trains** only these adapters (<1% of parameters)\n",
    "\n",
    "Result: 99% of the performance, 1% of the memory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d57cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx-lm\n",
      "  Downloading mlx_lm-0.28.4-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: mlx>=0.29.2 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (0.30.0)\n",
      "Requirement already satisfied: numpy in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (2.3.5)\n",
      "Requirement already satisfied: transformers>=4.39.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (4.57.3)\n",
      "  Downloading mlx_lm-0.28.4-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: mlx>=0.29.2 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (0.30.0)\n",
      "Requirement already satisfied: numpy in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (2.3.5)\n",
      "Requirement already satisfied: transformers>=4.39.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (4.57.3)\n",
      "Collecting sentencepiece (from mlx-lm)\n",
      "  Using cached sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting sentencepiece (from mlx-lm)\n",
      "  Using cached sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting protobuf (from mlx-lm)\n",
      "Collecting protobuf (from mlx-lm)\n",
      "  Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyyaml in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (6.0.3)\n",
      "Requirement already satisfied: jinja2 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (3.1.6)\n",
      "Requirement already satisfied: mlx-metal==0.30.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx>=0.29.2->mlx-lm) (0.30.0)\n",
      "Requirement already satisfied: filelock in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from jinja2->mlx-lm) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2025.11.12)\n",
      "  Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyyaml in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (6.0.3)\n",
      "Requirement already satisfied: jinja2 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (3.1.6)\n",
      "Requirement already satisfied: mlx-metal==0.30.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx>=0.29.2->mlx-lm) (0.30.0)\n",
      "Requirement already satisfied: filelock in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from jinja2->mlx-lm) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/markcastillo/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2025.11.12)\n",
      "Downloading mlx_lm-0.28.4-py3-none-any.whl (323 kB)\n",
      "Downloading mlx_lm-0.28.4-py3-none-any.whl (323 kB)\n",
      "Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Installing collected packages: sentencepiece, protobuf, mlx-lm\n",
      "\u001b[?25lInstalling collected packages: sentencepiece, protobuf, mlx-lm\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [mlx-lm]2m2/3\u001b[0m [mlx-lm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mlx-lm-0.28.4 protobuf-6.33.2 sentencepiece-0.2.1\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [mlx-lm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mlx-lm-0.28.4 protobuf-6.33.2 sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "# Install mlx-lm if not already installed\n",
    "!pip install mlx-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d1f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è  Hardware Acceleration Check:\n",
      "   Device: Device(gpu, 0)\n",
      "   ‚úÖ Using Apple Silicon GPU (Metal)\n",
      "   ‚ÑπÔ∏è  MLX automatically optimizes for the GPU's Unified Memory.\n",
      "   ‚ÑπÔ∏è  Note: While Apple Silicon has an NPU (Neural Engine), MLX primarily\n",
      "       uses the powerful GPU for general-purpose training tasks like LSTMs.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from mlx_lm import load, generate\n",
    "from mlx_nlp_utils import print_device_info\n",
    "\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d5b6a",
   "metadata": {},
   "source": [
    "## 1. Load a Pre-Trained Model\n",
    "\n",
    "We will use a small but capable model like **Mistral-7B** or **TinyLlama** (depending on your RAM). MLX handles the downloading automatically from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf5d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mlx-community/Llama-3.2-1B-Instruct-4bit...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c067a4f82fc40ab9587edf4f5cb4de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184fa49ee4964fa29053274e1367f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0727015d208f4c5dbc7e4a37abc90ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92fa6a43b7743a19530c8da9b07aed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313e258caa424c72b46477c0e99ddcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f5f0aaea3f47e09f571eced9d9ff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9acda6a60f4c4589e92d86f702a851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/695M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "# Using Llama-3.2-1B-Instruct (State of the art small model as of late 2024/2025)\n",
    "model_name = \"mlx-community/Llama-3.2-1B-Instruct-4bit\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7dea0",
   "metadata": {},
   "source": [
    "## 2. Test Base Model\n",
    "\n",
    "Let's see how it performs *before* fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff2230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 09 Dec 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "====================\n",
      "==========\n",
      "The capital ofThe capital of France is Paris France is Paris.\n",
      "==========\n",
      "Prompt: 42 tokens, 101.670 tokens-per-sec\n",
      "Generation: 8 tokens, 280.702 tokens-per-sec\n",
      "Peak memory: 0.755 GB\n",
      ".\n",
      "==========\n",
      "Prompt: 42 tokens, 101.670 tokens-per-sec\n",
      "Generation: 8 tokens, 280.702 tokens-per-sec\n",
      "Peak memory: 0.755 GB\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer's chat template (Modern Best Practice)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"Formatted Prompt:\\n{prompt}\\n{'='*20}\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1c03b",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Fine-Tuning\n",
    "\n",
    "We need data in a specific format (JSONL) for `mlx-lm`. Let's convert our Intent/Sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c21e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 128 examples to data/train.jsonl\n",
      "Saved 32 examples to data/valid.jsonl\n",
      "\n",
      "‚úÖ Data prepared for MLX LoRA!\n",
      "   Format: JSONL (Chat format)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load our existing Intent Classification data\n",
    "data_path = Path('../data/intent_samples/data.json')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"‚ö†Ô∏è Data not found. Please run: python ../scripts/download_datasets.py --samples\")\n",
    "else:\n",
    "    with open(data_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    texts = raw_data['texts']\n",
    "    labels = raw_data['labels']\n",
    "    \n",
    "    # 2. Convert to Chat Format (JSONL)\n",
    "    # We want the model to learn to classify intents.\n",
    "    # User: \"Turn on the lights\" -> Assistant: \"intent: command\"\n",
    "    \n",
    "    chat_data = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{text}'\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"intent: {label}\"}\n",
    "            ]\n",
    "        }\n",
    "        chat_data.append(entry)\n",
    "    \n",
    "    # 3. Save as train.jsonl and valid.jsonl\n",
    "    # Split 80/20\n",
    "    split_idx = int(len(chat_data) * 0.8)\n",
    "    train_data = chat_data[:split_idx]\n",
    "    valid_data = chat_data[split_idx:]\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    Path('data').mkdir(exist_ok=True)\n",
    "    \n",
    "    def save_jsonl(data, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "        print(f\"Saved {len(data)} examples to {filename}\")\n",
    "\n",
    "    save_jsonl(train_data, 'data/train.jsonl')\n",
    "    save_jsonl(valid_data, 'data/valid.jsonl')\n",
    "    \n",
    "    print(\"\\n‚úÖ Data prepared for MLX LoRA!\")\n",
    "    print(\"   Format: JSONL (Chat format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2373b",
   "metadata": {},
   "source": [
    "## 4. Run Fine-Tuning (LoRA)\n",
    "\n",
    "We can use the `mlx_lm.lora` command line tool or API to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Command (for reference):\n",
      "============================================================\n",
      "cd notebooks && python -m mlx_lm.lora \\\n",
      "    --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\n",
      "    --train \\\n",
      "    --data data \\\n",
      "    --batch-size 4 \\\n",
      "    --lora-layers 16 \\\n",
      "    --iters 600 \\\n",
      "    --learning-rate 1e-4 \\\n",
      "    --adapter-path ./adapters\n",
      "============================================================\n",
      "\n",
      "üí° Recommendation: Run the next cell to train in the notebook!\n",
      "   (It handles paths automatically)\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "# We use the CLI tool provided by mlx-lm\n",
    "# --batch-size 4: Fits easily on 8GB/16GB RAM\n",
    "# --num-layers 16: Target more layers for better quality\n",
    "# --iters 600: Enough for a small dataset\n",
    "\n",
    "print(\"üìã Training Command (for reference):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cmd = \"\"\"cd notebooks && python -m mlx_lm lora \\\\\n",
    "    --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\\\n",
    "    --train \\\\\n",
    "    --data data \\\\\n",
    "    --batch-size 4 \\\\\n",
    "    --num-layers 16 \\\\\n",
    "    --iters 600 \\\\\n",
    "    --learning-rate 1e-4 \\\\\n",
    "    --adapter-path ./adapters\"\"\"\n",
    "\n",
    "print(cmd)\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Recommendation: Run the next cell to train in the notebook!\")\n",
    "print(\"   (It handles paths automatically)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting LoRA Fine-Tuning...\n",
      "============================================================\n",
      "üìç Project Root: /Users/markcastillo/git/apple-mlx-tutorial\n",
      "üìÇ Data Dir: /Users/markcastillo/git/apple-mlx-tutorial/notebooks/data\n",
      "üíæ Adapter Dir: /Users/markcastillo/git/apple-mlx-tutorial/notebooks/adapters\n",
      "\n",
      "‚úÖ Data found. Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "usage: lora.py [-h] [--model MODEL] [--train] [--data DATA]\n",
      "               [--fine-tune-type {lora,dora,full}]\n",
      "               [--optimizer {adam,adamw,muon,sgd,adafactor}] [--mask-prompt]\n",
      "               [--num-layers NUM_LAYERS] [--batch-size BATCH_SIZE]\n",
      "               [--iters ITERS] [--val-batches VAL_BATCHES]\n",
      "               [--learning-rate LEARNING_RATE]\n",
      "               [--steps-per-report STEPS_PER_REPORT]\n",
      "               [--steps-per-eval STEPS_PER_EVAL]\n",
      "               [--grad-accumulation-steps GRAD_ACCUMULATION_STEPS]\n",
      "               [--resume-adapter-file RESUME_ADAPTER_FILE]\n",
      "               [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY]\n",
      "               [--test] [--test-batches TEST_BATCHES]\n",
      "               [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG]\n",
      "               [--grad-checkpoint] [--report-to REPORT_TO]\n",
      "               [--project-name PROJECT_NAME] [--seed SEED]\n",
      "lora.py: error: unrecognized arguments: --lora-layers 16\n",
      "\n",
      "‚úÖ Training Complete!\n",
      "   Adapters saved to: /Users/markcastillo/git/apple-mlx-tutorial/notebooks/adapters\n",
      "\n",
      "‚úÖ Training Complete!\n",
      "   Adapters saved to: /Users/markcastillo/git/apple-mlx-tutorial/notebooks/adapters\n"
     ]
    }
   ],
   "source": [
    "# Run LoRA Training (5-10 minutes on M1/M2/M3)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Disable tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"üöÄ Starting LoRA Fine-Tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Setup Paths\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Find project root and working directory\n",
    "if (current_dir / \"notebooks\").exists():\n",
    "    project_root = current_dir\n",
    "    working_dir = current_dir / \"notebooks\"\n",
    "elif current_dir.name == \"notebooks\":\n",
    "    project_root = current_dir.parent\n",
    "    working_dir = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "    working_dir = current_dir\n",
    "\n",
    "data_dir = working_dir / \"data\"\n",
    "adapter_dir = working_dir / \"adapters\"\n",
    "\n",
    "print(f\"üìç Project Root: {project_root}\")\n",
    "print(f\"üìÇ Data Dir: {data_dir}\")\n",
    "print(f\"üíæ Adapter Dir: {adapter_dir}\")\n",
    "\n",
    "# 2. Verify Data Exists\n",
    "if not (data_dir / \"train.jsonl\").exists():\n",
    "    print(\"\\n‚ùå ERROR: 'train.jsonl' not found in data directory.\")\n",
    "    print(\"   Please run the 'Prepare Data' cell above first!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data found. Starting training...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 3. Run Training Command\n",
    "    # Using the updated mlx_lm CLI syntax (2025)\n",
    "    !python -m mlx_lm lora \\\n",
    "        --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\n",
    "        --train \\\n",
    "        --data \"{data_dir}\" \\\n",
    "        --batch-size 4 \\\n",
    "        --num-layers 16 \\\n",
    "        --iters 100 \\\n",
    "        --learning-rate 1e-4 \\\n",
    "        --adapter-path \"{adapter_dir}\"\n",
    "    \n",
    "    print(\"\\n‚úÖ Training Complete!\")\n",
    "    print(f\"   Adapters saved to: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafae00",
   "metadata": {},
   "source": [
    "### What the Training Does\n",
    "\n",
    "The LoRA training will:\n",
    "1. Download the base model (if not cached)\n",
    "2. Load your training/validation data\n",
    "3. Add LoRA adapters to attention layers\n",
    "4. Train for 600 iterations (~5-10 minutes on M1/M2)\n",
    "5. Save the adapters to `./adapters`\n",
    "\n",
    "**Note**: This is a demo. For production:\n",
    "- Use more data (100+ examples minimum)\n",
    "- Train longer (1000-5000 iterations)\n",
    "- Tune hyperparameters (learning rate, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253bf00",
   "metadata": {},
   "source": [
    "## 5. Inference with Fine-Tuned Model\n",
    "\n",
    "After training completes, we can load the base model + adapters and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6dbfb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading fine-tuned model (base + LoRA adapters)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fe252f5011413a8556b06bf89ee19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[load_safetensors] Failed to open file adapters/adapters.safetensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_path.exists():\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loading fine-tuned model (base + LoRA adapters)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     model_ft, tokenizer_ft = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Fine-tuned model loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages/mlx_lm/utils.py:324\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path_or_hf_repo, tokenizer_config, model_config, adapter_path, lazy, return_config, revision)\u001b[39m\n\u001b[32m    322\u001b[39m model, config = load_model(model_path, lazy, model_config=model_config)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     model = \u001b[43mload_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     model.eval()\n\u001b[32m    326\u001b[39m tokenizer = load_tokenizer(\n\u001b[32m    327\u001b[39m     model_path, tokenizer_config, eos_token_ids=config.get(\u001b[33m\"\u001b[39m\u001b[33meos_token_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    328\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages/mlx_lm/utils.py:259\u001b[39m, in \u001b[36mload_adapters\u001b[39m\u001b[34m(model, adapter_path)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_adapters\u001b[39m(model: nn.Module, adapter_path: \u001b[38;5;28mstr\u001b[39m) -> nn.Module:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuner\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_adapters \u001b[38;5;28;01mas\u001b[39;00m _load_adapters\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages/mlx_lm/tuner/utils.py:137\u001b[39m, in \u001b[36mload_adapters\u001b[39m\u001b[34m(model, adapter_path)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fine_tune_type != \u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    131\u001b[39m     linear_to_lora_layers(\n\u001b[32m    132\u001b[39m         model,\n\u001b[32m    133\u001b[39m         config.num_layers,\n\u001b[32m    134\u001b[39m         config.lora_parameters,\n\u001b[32m    135\u001b[39m         use_dora=(fine_tune_type == \u001b[33m\"\u001b[39m\u001b[33mdora\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    136\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madapters.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py:177\u001b[39m, in \u001b[36mModule.load_weights\u001b[39m\u001b[34m(self, file_or_weights, strict)\u001b[39m\n\u001b[32m    175\u001b[39m weights = file_or_weights\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     weights = \u001b[38;5;28mlist\u001b[39m(\u001b[43mmx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m.items())\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[32m    180\u001b[39m     new_weights = \u001b[38;5;28mdict\u001b[39m(weights)\n",
      "\u001b[31mRuntimeError\u001b[39m: [load_safetensors] Failed to open file adapters/adapters.safetensors"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model (base + adapters) if available\n",
    "from pathlib import Path\n",
    "\n",
    "adapter_path = Path(\"./adapters\")\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(\"‚úÖ Loading fine-tuned model (base + LoRA adapters)...\")\n",
    "    model_ft, tokenizer_ft = load(model_name, adapter_path=str(adapter_path))\n",
    "    print(\"‚úÖ Fine-tuned model loaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No adapters found. Will use base model only.\")\n",
    "    print(\"   (Run the training command first to create adapters)\")\n",
    "    model_ft, tokenizer_ft = model, tokenizer\n",
    "\n",
    "# Side-by-side comparison\n",
    "test_query = \"Turn off the lights\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{test_query}'\"}\n",
    "]\n",
    "prompt_test = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüì¶ BASE MODEL:\")\n",
    "response_base = generate(model, tokenizer, prompt=prompt_test, max_tokens=50, verbose=False)\n",
    "print(response_base)\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(\"\\nüéØ FINE-TUNED MODEL:\")\n",
    "    response_ft = generate(model_ft, tokenizer_ft, prompt=prompt_test, max_tokens=50, verbose=False)\n",
    "    print(response_ft)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Fine-tuned model not available (run training first)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Expected Improvement:\")\n",
    "print(\"   - Base model: Generic response or incorrect intent\")\n",
    "print(\"   - Fine-tuned: Correctly identifies 'intent: command'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f046f",
   "metadata": {},
   "source": [
    "## 6. Compare: Base vs Fine-Tuned\n",
    "\n",
    "Let's see the difference between the base model and our fine-tuned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing Fine-Tuned Model\n",
      "==================================================\n",
      "\n",
      "üìù Input: 'Turn on the light'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'What time is it'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'Hello there'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "üìù Input: 'Set a timer for 5 minutes'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'Hello there'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "üìù Input: 'Set a timer for 5 minutes'\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'How are you doing'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "==================================================\n",
      "ü§ñ Output: intent: question\n",
      "\n",
      "üìù Input: 'How are you doing'\n",
      "ü§ñ Output: intent: greeting\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on intent classification\n",
    "if not adapter_path.exists():\n",
    "    print(\"‚ö†Ô∏è  No fine-tuned model available. Run training first.\")\n",
    "    print(\"   Using base model for testing.\")\n",
    "\n",
    "test_cases = [\n",
    "    \"Turn on the light\",\n",
    "    \"What time is it\",\n",
    "    \"Hello there\",\n",
    "    \"Set a timer for 5 minutes\",\n",
    "    \"How are you doing\"\n",
    "]\n",
    "\n",
    "model_to_test = model_ft if adapter_path.exists() else model\n",
    "tokenizer_to_test = tokenizer_ft if adapter_path.exists() else tokenizer\n",
    "model_type = \"Fine-Tuned\" if adapter_path.exists() else \"Base\"\n",
    "\n",
    "print(f\"\\nüß™ Testing {model_type} Model\\n\" + \"=\"*50)\n",
    "\n",
    "for test_text in test_cases:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{test_text}'\"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer_to_test.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    print(f\"\\nüìù Input: '{test_text}'\")\n",
    "    response = generate(model_to_test, tokenizer_to_test, prompt=prompt, max_tokens=50, verbose=False)\n",
    "    print(f\"ü§ñ Output: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cd30e",
   "metadata": {},
   "source": [
    "## Further Experiments: Optimizing LoRA\n",
    "\n",
    "Fine-tuning is an art as much as a science. Here are some experiments to deepen your understanding.\n",
    "\n",
    "### Experiment: Finding the Optimal Rank\n",
    "We used `lora_rank=8`. What happens if we change it?\n",
    "- **Rank 4:** Faster, less memory, but might not learn complex patterns.\n",
    "- **Rank 32:** Slower, more memory, potentially better quality.\n",
    "- **Rank 64+:** Diminishing returns, high risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Find optimal LoRA rank\n",
    "\n",
    "def test_lora_ranks():\n",
    "    \"\"\"Compare different LoRA ranks on same task\"\"\"\n",
    "    print(\"Testing LoRA Ranks...\")\n",
    "    \n",
    "    ranks = [4, 8, 16, 32]\n",
    "    \n",
    "    # In a real experiment, we would loop through these ranks, \n",
    "    # train a model for each, and evaluate on a validation set.\n",
    "    \n",
    "    for rank in ranks:\n",
    "        print(f\"Training with Rank {rank}...\")\n",
    "        # train_lora(rank=rank)\n",
    "        # score = evaluate()\n",
    "        # print(f\"Rank {rank} Score: {score}\")\n",
    "        pass\n",
    "        \n",
    "    print(\"General Rule: Rank 8 or 16 is usually the sweet spot for 7B models.\")\n",
    "\n",
    "# test_lora_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7751",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "You have now completed the journey from simple RNNs to fine-tuning modern LLMs on Apple Silicon!\n",
    "\n",
    "**What's Next?**\n",
    "- Build a RAG (Retrieval Augmented Generation) system.\n",
    "- Deploy your fine-tuned model as an API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c62467",
   "metadata": {},
   "source": [
    "## ‚ùì FAQ\n",
    "\n",
    "**Q: LoRA vs. Full Fine-Tuning?**\n",
    "A:\n",
    "*   **Full Fine-Tuning:** Updates all weights. Requires massive VRAM (e.g., 80GB+ for 7B model).\n",
    "*   **LoRA:** Updates <1% of weights. Runs on consumer hardware (e.g., 16GB MacBook). Performance is often 98-99% of full fine-tuning.\n",
    "\n",
    "**Q: Can I fine-tune on my own emails?**\n",
    "A: Yes! Just format them as JSONL: `{\"messages\": [{\"role\": \"user\", \"content\": \"Subject: Meeting\"}, {\"role\": \"assistant\", \"content\": \"Hi team...\"}]}`.\n",
    "\n",
    "**Q: What is \"Quantization\" (4-bit)?**\n",
    "A: It reduces the precision of weights from 16-bit (Float16) to 4-bit integers. This cuts memory usage by 4x with minimal loss in quality, allowing you to run a 7B model on a laptop.\n",
    "\n",
    "## üí≠ Closing Thoughts\n",
    "\n",
    "**The Commoditization of Intelligence**\n",
    "We are entering an era where \"Intelligence\" is a downloadable package.\n",
    "*   **2020:** Only OpenAI had GPT-3.\n",
    "*   **2025:** You can run a model nearly as smart as GPT-3 on your laptop, fine-tuned on your private data, with no internet connection.\n",
    "\n",
    "**Architectural Shift:**\n",
    "The role of the AI Engineer is shifting from \"Designing Architectures\" (building LSTMs) to \"Data Curation\" (preparing high-quality datasets for fine-tuning). The model is a solved problem; the data is your competitive advantage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
