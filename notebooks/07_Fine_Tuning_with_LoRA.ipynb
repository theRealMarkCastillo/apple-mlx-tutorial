{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce8ca1d",
   "metadata": {},
   "source": [
    "# üéì Fine-Tuning LLMs with LoRA on Apple Silicon\n",
    "\n",
    "## What You'll Learn\n",
    "- **LoRA (Low-Rank Adaptation)**: Train a 1B parameter model on a laptop\n",
    "- **Quantization**: Run models 4x smaller with 4-bit precision\n",
    "- **MLX-LM**: Apple's unified interface for LLMs\n",
    "- **Production Ready**: Chat templates and modern best practices (2025)\n",
    "\n",
    "## The Problem: Traditional Fine-Tuning is Expensive\n",
    "\n",
    "Training a 7B parameter model requires:\n",
    "- **Full Fine-Tuning**: 80GB+ VRAM, $10,000+ GPU\n",
    "- **LoRA**: 16GB RAM, Your MacBook (Free!)\n",
    "\n",
    "## The Solution: LoRA\n",
    "\n",
    "Instead of updating all 7 billion parameters, LoRA:\n",
    "1. **Freezes** the original model weights\n",
    "2. **Adds** small \"adapter\" matrices (rank-deficient)\n",
    "3. **Trains** only these adapters (<1% of parameters)\n",
    "\n",
    "Result: 99% of the performance, 1% of the memory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d57cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx-lm\n",
      "  Using cached mlx_lm-0.28.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: mlx>=0.29.2 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (0.29.4)\n",
      "Requirement already satisfied: numpy in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (2.3.5)\n",
      "Collecting transformers>=4.39.3 (from mlx-lm)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting transformers>=4.39.3 (from mlx-lm)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting protobuf (from mlx-lm)\n",
      "Collecting protobuf (from mlx-lm)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyyaml in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (6.0.3)\n",
      "Requirement already satisfied: jinja2 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (3.1.6)\n",
      "Requirement already satisfied: mlx-metal==0.29.4 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx>=0.29.2->mlx-lm) (0.29.4)\n",
      "Requirement already satisfied: filelock in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (3.20.0)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyyaml in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (6.0.3)\n",
      "Requirement already satisfied: jinja2 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx-lm) (3.1.6)\n",
      "Requirement already satisfied: mlx-metal==0.29.4 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from mlx>=0.29.2->mlx-lm) (0.29.4)\n",
      "Requirement already satisfied: filelock in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.39.3->mlx-lm)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.39.3->mlx-lm)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (25.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->mlx-lm)\n",
      "  Using cached regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2.32.5)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->mlx-lm)\n",
      "  Using cached regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.39.3->mlx-lm)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.39.3->mlx-lm)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.39.3->mlx-lm)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.39.3->mlx-lm)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from jinja2->mlx-lm) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2025.11.12)\n",
      "Using cached mlx_lm-0.28.3-py3-none-any.whl (294 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.39.3->mlx-lm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from jinja2->mlx-lm) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mark/git/apple-mlx-tutorial/.venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2025.11.12)\n",
      "Using cached mlx_lm-0.28.3-py3-none-any.whl (294 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Using cached regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Installing collected packages: safetensors, regex, protobuf, huggingface-hub, tokenizers, transformers, mlx-lm\n",
      "\u001b[?25lInstalling collected packages: safetensors, regex, protobuf, huggingface-hub, tokenizers, transformers, mlx-lm\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.1.4\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.1.4:\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.1.4\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.1.4\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.1.4:\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.1.4\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7/7\u001b[0m [mlx-lm]2m6/7\u001b[0m [mlx-lm]rmers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.36.0 mlx-lm-0.28.3 protobuf-6.33.1 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.1\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7/7\u001b[0m [mlx-lm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.36.0 mlx-lm-0.28.3 protobuf-6.33.1 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "# Install mlx-lm if not already installed\n",
    "!pip install mlx-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d1f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è  Hardware Acceleration Check:\n",
      "   Device: Device(gpu, 0)\n",
      "   ‚úÖ Using Apple Silicon GPU (Metal)\n",
      "   ‚ÑπÔ∏è  MLX automatically optimizes for the GPU's Unified Memory.\n",
      "   ‚ÑπÔ∏è  Note: While Apple Silicon has an NPU (Neural Engine), MLX primarily\n",
      "       uses the powerful GPU for general-purpose training tasks like LSTMs.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from mlx_lm import load, generate\n",
    "from mlx_nlp_utils import print_device_info\n",
    "\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d5b6a",
   "metadata": {},
   "source": [
    "## 1. Load a Pre-Trained Model\n",
    "\n",
    "We will use a small but capable model like **Mistral-7B** or **TinyLlama** (depending on your RAM). MLX handles the downloading automatically from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf5d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mlx-community/Llama-3.2-1B-Instruct-4bit...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dfbab06dc0483d90b9b474e7e0fa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd31ba8291f4be9b49bc64ced43cab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a0034585114802a422178e6f40a60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dd579421d142ff9e9e61e1c064935d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfad77d5bfa47119592f663853e3e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb2d303ec9f4055aaf39cdab73b6a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/695M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06fbaf32fe341809c983fd949b04d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "# Using Llama-3.2-1B-Instruct (State of the art small model as of late 2024/2025)\n",
    "model_name = \"mlx-community/Llama-3.2-1B-Instruct-4bit\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7dea0",
   "metadata": {},
   "source": [
    "## 2. Test Base Model\n",
    "\n",
    "Let's see how it performs *before* fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff2230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 19 Nov 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "====================\n",
      "==========\n",
      "The capitalThe capital of France of France is Paris is Paris..\n",
      "==========\n",
      "Prompt: 42 tokens, 85.655 tokens-per-sec\n",
      "Generation: 8 tokens, 149.560 tokens-per-sec\n",
      "Peak memory: 0.755 GB\n",
      "\n",
      "==========\n",
      "Prompt: 42 tokens, 85.655 tokens-per-sec\n",
      "Generation: 8 tokens, 149.560 tokens-per-sec\n",
      "Peak memory: 0.755 GB\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer's chat template (Modern Best Practice)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"Formatted Prompt:\\n{prompt}\\n{'='*20}\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1c03b",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Fine-Tuning\n",
    "\n",
    "We need data in a specific format (JSONL) for `mlx-lm`. Let's convert our Intent/Sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c21e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7 examples to data/train.jsonl\n",
      "Saved 2 examples to data/valid.jsonl\n",
      "\n",
      "‚úÖ Data prepared for MLX LoRA!\n",
      "   Format: JSONL (Chat format)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load our existing Intent Classification data\n",
    "data_path = Path('../data/intent_samples/data.json')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"‚ö†Ô∏è Data not found. Please run: python ../scripts/download_datasets.py --samples\")\n",
    "else:\n",
    "    with open(data_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    texts = raw_data['texts']\n",
    "    labels = raw_data['labels']\n",
    "    \n",
    "    # 2. Convert to Chat Format (JSONL)\n",
    "    # We want the model to learn to classify intents.\n",
    "    # User: \"Turn on the lights\" -> Assistant: \"intent: command\"\n",
    "    \n",
    "    chat_data = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{text}'\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"intent: {label}\"}\n",
    "            ]\n",
    "        }\n",
    "        chat_data.append(entry)\n",
    "    \n",
    "    # 3. Save as train.jsonl and valid.jsonl\n",
    "    # Split 80/20\n",
    "    split_idx = int(len(chat_data) * 0.8)\n",
    "    train_data = chat_data[:split_idx]\n",
    "    valid_data = chat_data[split_idx:]\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    Path('data').mkdir(exist_ok=True)\n",
    "    \n",
    "    def save_jsonl(data, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "        print(f\"Saved {len(data)} examples to {filename}\")\n",
    "\n",
    "    save_jsonl(train_data, 'data/train.jsonl')\n",
    "    save_jsonl(valid_data, 'data/valid.jsonl')\n",
    "    \n",
    "    print(\"\\n‚úÖ Data prepared for MLX LoRA!\")\n",
    "    print(\"   Format: JSONL (Chat format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2373b",
   "metadata": {},
   "source": [
    "## 4. Run Fine-Tuning (LoRA)\n",
    "\n",
    "We can use the `mlx_lm.lora` command line tool or API to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a11e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this command in your terminal:\n",
      "\n",
      "python -m mlx_lm.lora \\\n",
      "    --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\n",
      "    --train \\\n",
      "    --data data \\\n",
      "    --batch-size 4 \\\n",
      "    --lora-layers 16 \\\n",
      "    --iters 600 \\\n",
      "    --learning-rate 1e-4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example command to run fine-tuning\n",
    "# We use the CLI tool provided by mlx-lm\n",
    "# --batch-size 4: Fits easily on 8GB/16GB RAM\n",
    "# --lora-layers 16: Target more layers for better quality\n",
    "# --iters 600: Enough for a small dataset\n",
    "\n",
    "cmd = \"\"\"\n",
    "python -m mlx_lm.lora \\\\\n",
    "    --model mlx-community/Llama-3.2-1B-Instruct-4bit \\\\\n",
    "    --train \\\\\n",
    "    --data data \\\\\n",
    "    --batch-size 4 \\\\\n",
    "    --lora-layers 16 \\\\\n",
    "    --iters 600 \\\\\n",
    "    --learning-rate 1e-4\n",
    "\"\"\"\n",
    "\n",
    "print(\"Run this command in your terminal:\")\n",
    "print(cmd)\n",
    "\n",
    "# Note: You can also run it directly here with !python ...\n",
    "# !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Run training directly in notebook\n",
    "# Uncomment to train (this will take 5-10 minutes)\n",
    "\n",
    "# import subprocess\n",
    "# result = subprocess.run([\n",
    "#     \"python\", \"-m\", \"mlx_lm.lora\",\n",
    "#     \"--model\", \"mlx-community/Llama-3.2-1B-Instruct-4bit\",\n",
    "#     \"--train\",\n",
    "#     \"--data\", \"data\",\n",
    "#     \"--batch-size\", \"4\",\n",
    "#     \"--lora-layers\", \"16\", \n",
    "#     \"--iters\", \"600\",\n",
    "#     \"--learning-rate\", \"1e-4\",\n",
    "#     \"--adapter-path\", \"./adapters\"\n",
    "# ], capture_output=True, text=True)\n",
    "# \n",
    "# print(result.stdout)\n",
    "# print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafae00",
   "metadata": {},
   "source": [
    "### What the Training Does\n",
    "\n",
    "The LoRA training will:\n",
    "1. Download the base model (if not cached)\n",
    "2. Load your training/validation data\n",
    "3. Add LoRA adapters to attention layers\n",
    "4. Train for 600 iterations (~5-10 minutes on M1/M2)\n",
    "5. Save the adapters to `./adapters`\n",
    "\n",
    "**Note**: This is a demo. For production:\n",
    "- Use more data (100+ examples minimum)\n",
    "- Train longer (1000-5000 iterations)\n",
    "- Tune hyperparameters (learning rate, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253bf00",
   "metadata": {},
   "source": [
    "## 5. Inference with Fine-Tuned Model\n",
    "\n",
    "After training completes, we can load the base model + adapters and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "test_query = \"Turn off the lights\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{test_query}'\"}\n",
    "]\n",
    "prompt_test = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"Test Query: '{test_query}'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüì¶ BASE MODEL:\")\n",
    "response_base = generate(model, tokenizer, prompt=prompt_test, max_tokens=50, verbose=False)\n",
    "print(response_base)\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(\"\\nüéØ FINE-TUNED MODEL:\")\n",
    "    response_ft = generate(model_ft, tokenizer_ft, prompt=prompt_test, max_tokens=50, verbose=False)\n",
    "    print(response_ft)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Fine-tuned model not available (run training first)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Expected Improvement:\")\n",
    "print(\"   - Base model: Generic response or incorrect intent\")\n",
    "print(\"   - Fine-tuned: Correctly identifies 'intent: command'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f046f",
   "metadata": {},
   "source": [
    "## 6. Compare: Base vs Fine-Tuned\n",
    "\n",
    "Let's see the difference between the base model and our fine-tuned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on intent classification\n",
    "test_cases = [\n",
    "    \"Turn on the light\",\n",
    "    \"What time is it\",\n",
    "    \"Hello there\",\n",
    "    \"Set a timer for 5 minutes\",\n",
    "    \"How are you doing\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Fine-Tuned Model\\n\" + \"=\"*50)\n",
    "\n",
    "for test_text in test_cases:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the intent of this text: '{test_text}'\"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer_ft.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    print(f\"\\nüìù Input: '{test_text}'\")\n",
    "    response = generate(model_ft, tokenizer_ft, prompt=prompt, max_tokens=50, verbose=False)\n",
    "    print(f\"ü§ñ Output: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model (base + adapters)\n",
    "# If you haven't trained yet, this will just load the base model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "adapter_path = Path(\"./adapters\")\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(\"‚úÖ Loading fine-tuned model (base + LoRA adapters)...\")\n",
    "    model_ft, tokenizer_ft = load(model_name, adapter_path=str(adapter_path))\n",
    "    print(\"‚úÖ Fine-tuned model loaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No adapters found. Using base model.\")\n",
    "    print(\"   (Run the training command first to create adapters)\")\n",
    "    model_ft, tokenizer_ft = model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7751",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "You have now completed the journey from simple RNNs to fine-tuning modern LLMs on Apple Silicon!\n",
    "\n",
    "**What's Next?**\n",
    "- Build a RAG (Retrieval Augmented Generation) system.\n",
    "- Deploy your fine-tuned model as an API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c62467",
   "metadata": {},
   "source": [
    "## ‚ùì FAQ\n",
    "\n",
    "**Q: LoRA vs. Full Fine-Tuning?**\n",
    "A:\n",
    "*   **Full Fine-Tuning:** Updates all weights. Requires massive VRAM (e.g., 80GB+ for 7B model).\n",
    "*   **LoRA:** Updates <1% of weights. Runs on consumer hardware (e.g., 16GB MacBook). Performance is often 98-99% of full fine-tuning.\n",
    "\n",
    "**Q: Can I fine-tune on my own emails?**\n",
    "A: Yes! Just format them as JSONL: `{\"messages\": [{\"role\": \"user\", \"content\": \"Subject: Meeting\"}, {\"role\": \"assistant\", \"content\": \"Hi team...\"}]}`.\n",
    "\n",
    "**Q: What is \"Quantization\" (4-bit)?**\n",
    "A: It reduces the precision of weights from 16-bit (Float16) to 4-bit integers. This cuts memory usage by 4x with minimal loss in quality, allowing you to run a 7B model on a laptop.\n",
    "\n",
    "## üí≠ Closing Thoughts\n",
    "\n",
    "**The Commoditization of Intelligence**\n",
    "We are entering an era where \"Intelligence\" is a downloadable package.\n",
    "*   **2020:** Only OpenAI had GPT-3.\n",
    "*   **2025:** You can run a model nearly as smart as GPT-3 on your laptop, fine-tuned on your private data, with no internet connection.\n",
    "\n",
    "**Architectural Shift:**\n",
    "The role of the AI Engineer is shifting from \"Designing Architectures\" (building LSTMs) to \"Data Curation\" (preparing high-quality datasets for fine-tuning). The model is a solved problem; the data is your competitive advantage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
